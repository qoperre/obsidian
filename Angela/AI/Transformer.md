트랜스포머(Transformer)는 2017년 구글이 발표한 딥러닝 모델로, 특히 자연어 처리(NLP) 분야에 혁신을 가져온 아키텍처이다. 기존의 순차적 데이터 처리 방식의 한계를 극복하고 병렬 처리와 'Self-Attention' 메커니즘을 도입하여 뛰어난 성능을 보여주었다. 이는 ChatGPT와 같은 대규모 언어 모델(LLM)의 기반이 되었다.

### Self-Attention

트랜스포머의 가장 중요한 특징은 **셀프 어텐션(Self-Attention)** 메커니즘이다. 이는 문장 안의 단어들이 서로 어떤 관계를 맺고 있는지, 어떤 단어에 더 집중해야 하는지를 파악하는 방식이다. 예를 들어, "그 동물이 길을 건너지 않은 이유는 너무 피곤했기 때문이다"라는 문장에서 '그것'이 '동물'을 가리킨다는 것을 파악하는 것처럼, 단어 간의 관계와 중요도를 계산하여 문맥을 더 잘 이해하게 한다.

### 기존 모델(RNN, LSTM)과의 차이점

트랜스포머 이전에는 주로 순환 신경망(RNN)이나 LSTM 같은 모델이 사용되었다. 이들 모델은 데이터를 순차적으로 처리하기 때문에 문장이 길어질수록 앞부분의 정보를 잊어버리는 '장기 의존성 문제'가 있었다.

*   **처리 방식:** RNN, LSTM은 데이터를 순서대로 하나씩 처리하지만, 트랜스포머는 문장의 모든 단어를 동시에 병렬로 처리하여 학습 속도가 훨씬 빠르다.
*   **장기 의존성 문제 해결:** 셀프 어텐션 메커니즘을 통해 문장 내 모든 단어 간의 관계를 직접 계산하므로, 아무리 긴 문장이라도 단어 간의 관계를 효과적으로 파악할 수 있다.

### 구조

트랜스포머는 크게 **인코더(Encoder)** 와 **디코더(Decoder)** 두 부분으로 구성된다.

*   **인코더:** 입력된 문장의 의미와 문맥 정보를 압축하여 벡터 형태로 만든다. 셀프 어텐션 층과 피드 포워드 신경망으로 구성되어 있다.
*   **디코더:** 인코더가 압축한 정보를 바탕으로 번역이나 다음 문장 생성과 같은 결과물을 만들어낸다. 디코더 역시 셀프 어텐션, 인코더-디코더 어텐션, 피드 포워드 신경망을 포함한다.

## Self-Attention 과정

### 1. Input Embedding

컴퓨터는 단어를 직접 이해할 수 없으므로, 각 단어를 고유한 벡터(숫자의 배열)로 변환하는 과정이 필요하다. 이를 '임베딩'이라고 한다. 예를 들어, "I am a student"라는 문장이 있다면 각 단어는 특정 차원을 가진 벡터로 변환된다.

*   입력 행렬 $X$: 문장의 각 단어 임베딩 벡터를 쌓아서 만든 행렬

### 2. Positional Encoding

트랜스포머는 RNN과 달리 단어를 순차적으로 처리하지 않고 한 번에 전체 문장을 처리한다. 이로 인해 단어의 원래 위치 정보가 사라지는 문제가 발생한다. 이를 해결하기 위해 각 단어의 위치 정보를 담은 'Positional Encoding Vector'을을 만들어 기존 임베딩 벡터에 더해준다.

Positional Encoding은 주로 sine과 cosine 함수를 사용하여 계산된다.

*   `pos`: 문장 내 단어의 위치 (0부터 시작)
*   `i`: 임베딩 벡터 내의 차원 인덱스
*   $d_{\text{model}}$: 임베딩 벡터의 총 차원

$$
PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{\text{model}}})
$$
$$
PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{\text{model}}})
$$

이렇게 계산된 위치 인코딩 벡터가 입력 임베딩 벡터 $X$에 더해져 모델의 최종 입력이 된다.

### 3. Self-Attention

Self-Attention은 트랜스포머의 가장 핵심적인 부분으로, 문장 내에서 특정 단어가 다른 모든 단어와 어떤 연관성을 갖는지 점수를 계산하는 과정이다.

#### 3-1. Query, Key, Value 벡터 생성

먼저, 각 단어의 임베딩 벡터(위치 인코딩이 더해진)로부터 쿼리(Q), 키(K), 밸류(V)라는 세 가지 벡터를 생성한다. 이는 각각 학습 가능한 가중치 행렬 $W_Q$, $W_K$, $W_V$를 곱하여 얻어진다.

$$
Q = X W_Q
$$
$$
K = X W_K
$$
$$
V = X W_V
$$

-   **쿼리(Q):** 현재 처리 중인 단어의 벡터. 다른 단어와의 관련성을 측정하는 '질문' 역할을 한다.
-   **키(K):** 문장 내 모든 단어의 특성을 나타내는 벡터. 쿼리와의 유사도를 계산하는 '라벨' 역할을 한다.
-   **밸류(V):** 문장 내 모든 단어의 실제 의미를 담은 벡터. 어텐션 점수에 따라 가중합될 '값'이다.

#### 3-2. 어텐션 점수(Attention Score) 계산

특정 단어의 '쿼리' 벡터와 문장 내 모든 단어의 '키' 벡터를 내적하여 유사도, 즉 어텐션 점수를 계산한다.

$$
\text{Scores} = Q K^T
$$

#### 3-3. Scaling

내적 값이 너무 커지면 그래디언트 소실 문제가 발생할 수 있어, 키 벡터 차원 $d_k$의 제곱근으로 나누어 값을 안정화시킨다.

$$
\text{Scaled Scores} = \frac{Q K^T}{\sqrt{d_k}}
$$

#### 3-4. Softmax

스케일링된 점수에 소프트맥스 함수를 적용하여 총합이 1이 되는 확률 분포, 즉 '어텐션 가중치(Attention Weights)'를 얻는다. 이 값은 각 단어가 현재 단어에 얼마나 중요한지를 나타낸다.

$$
\text{Attention Weights} = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right)
$$

#### 3-5. 최종 어텐션 출력 계산

마지막으로, 계산된 어텐션 가중치와 '밸류(V)' 벡터를 가중합하여 최종 어텐션 출력을 얻는다. 이는 문맥 정보가 풍부하게 반영된 새로운 벡터 표현이다.

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right)V
$$

### 4. 멀티-헤드 어텐션 (Multi-Head Attention)

트랜스포머는 한 번의 어텐션을 수행하는 대신, 여러 개의 어텐션(헤드)을 병렬로 수행하는 '멀티-헤드 어텐션'을 사용한다. 이는 마치 여러 관점에서 문장을 동시에 바라보는 것과 같아서, 단어 간의 다양한 관계(예: 문법적 관계, 의미적 관계 등)를 효과적으로 포착할 수 있게 한다.

각 헤드는 독자적인 $W_i^Q$, $W_i^K$, $W_i^V$ 가중치를 가지며, 독립적으로 어텐션 출력을 계산한다. 이렇게 얻어진 여러 개의 어텐션 출력들을 하나로 연결(concatenate)한 후, 또 다른 가중치 행렬 $W^O$를 곱해 최종 출력을 만든다.

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W^O
$$
$$
\text{where head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$
