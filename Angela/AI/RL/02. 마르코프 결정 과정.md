bandit problem에서는 agent가 어떤 action을 취하든 environment의 설정은 변하지 않음. agent는 설정을 바꿀 수 없는 bnadit을 반복해서 플레이하면서 그 중 가장 value가 높은 bandit을 찾는다. 그러나 바둑과 같은 문제의 경우, 어떤 수를 두면 바둑판 위의 돌 배치(environment)가 달라진다. 그리고 상대가 돌을 두면 또 달라진다. 이처럼 agent의 action에 따라 envirionment가 변하는 문제들이 대부분이고, 따라서 agent는 envirionment가 변하는 것을 고려하여 최선의 수를 두어야 한다.
* *\*1장에서는 bandit의 reward 설정(reward의 확률 분포)이 시간에 따라 변하는 non-stational plroblem도 있었다. 그러나 non-stational problem에서의 reward의 'probablity distribution'은 (agent가 어떤 action을 취하는지와는 관계 없이)시간에 따라서만 변화한다. 그러나 이번 장에서는 'agent의 action에 따라'envirionment의 state가 변하는 문제를 다룬다.

이러한 문제의 대표적인 예로 Marcov Decision Process(MDP)이 있다. 먼저 MDP의 목표를 명확히 규정한 후, 간단한 MDP 문제를 풀어 목표를 달성하는 과정을 살펴보자.

# 2.1 MDP란?

MDP에서 Decision Problem이란 'agent가(environment와 interact하면서) action을 취하는 과정'을 뜻한다.

## 2.1.1 Example
아래 그림을 보면 세상은 grid로 구분되어 있고, 그 안에 로봇(agent)가 있다. agent는 오른쪽 또는 왼쪽으로 이동할 수 있다. 이 글에서는 이와 같은 세계를 'grid world'라고 부르기로 하겠다.
```sheet
{
  classes: {
    center: { textAlign: "center", verticalAlign: "middle", padding: "15px" },
    noBorder: { border: "none", backgroundColor: "transparent" },
    redArrow: { color: "red", fontWeight: "bold", fontSize: "22px", textAlign: "right", paddingRight: "0px" },
    c: {backgroundColor: "#aaa"}
  }
}
---
|-| +1 ~.center .noBorder | ~.noBorder   | ← ● ● → ~.center .noBorder |  ~.noBorder  | -2 ~.center .noBorder |
|-| :-------------------: | :----------: | :-----------------------: | :----------: | :-------------------: |
|- ~.redArrow| 🍎 ~.center .c        |  ~.c            | 👾 ~.center .c  .redArrow            |     ~.c       | 💣 ~.center .c          |

```
로봇이 agent이고, 주변이 environment이다. agent는 오른쪽으로 이동하거나 왼쪽으로 이동하는 두 가지 행동을 취할 수 있다. 또한 그림에서의 가장 왼쪽 칸에는 사과가 있고 오른쪽 칸에는 폭탄이 있다. 이들은 agent에게 주어지는 reward이다. 사과를 얻을 때의 reward는 +1로, 폭탄을 얻을 때의 reward는 -2로 하겠다. 빈칸의 reward는 0이다.

이 문제에서는 agent가 행동할 때마다 상황이 변한다. 예를 들어 왼쪽으로 두 번 연달아 이동했다고 가정해보자. 그러면 agent는 +1의 reward를 얻는다. 반대로 오른쪽으로 두 번 이동하면 -2의 reward를 받는다. 이러한 agent의 이동을 다음과 같이 나타낼 수 있다.

```sheet
{
  classes: {
    center: { textAlign: "center", verticalAlign: "middle", padding: "14px" },
    sp: { border: "none", backgroundColor: "transparent", minWidth: "80px" },
    noBorder: { border: "none", backgroundColor: "transparent" },
    invis: { color: "transparent" },
    c: { backgroundColor: "#aaa" },
    bigArrow: { fontSize: "30px", fontWeight: "bold" },
    vCloser: { verticalAlign: "bottom", paddingTop: "2px", paddingBottom: "0px" },
    redArrow: { color: "red", fontWeight: "bold", fontSize: "22px", textAlign: "right", paddingRight: "0px" },
    bold: { fontWeight: "bold" }
  }
}
---
|- ~.redArrow| · ~.invis .noBorder | · ~.invis .noBorder | · ~.invis .noBorder | 🍎 ~.center .c | · ~.invis .c | 👾 ~.center .c .redArrow | · ~.invis .c | 💣 ~.center .c | · ~.invis .noBorder | · ~.invis .noBorder | · ~.invis .noBorder |
|-| :--: | :--: | :--: | :--: | :--: | :---: ~.sp | :--: | :--: | :--: | :--: | :--: |
|- ~.noBorder| · ~.invis | · ~.invis | · ~.invis | · ~.invis | ↙️ ~.center .bigArrow | · ~.sp .invis | ↘️ ~.center .bigArrow | · ~.invis | · ~.invis | · ~.invis | · ~.invis |
|- ~.c .redArrow| 🍎 ~.center | 👾 ~.center .redArrow | · ~.invis | · ~.invis | 💣 ~.center | · ~.sp .invis | 🍎 ~.center | · ~.invis | · ~.invis | 👾 ~.center .redArrow | 💣 ~.center |
|- ~.noBorder| +1 ~.center .vCloser .bold | · ~.invis | ↓ ~.center .bigArrow | · ~.invis | · ~.invis | · ~.sp .invis | · ~.invis | · ~.invis | ↓ ~.center .bigArrow | · ~.invis | -2 ~.center .vCloser .bold |
|- ~.c .redArrow| 👾🍎 ~.center | · ~.invis | · ~.invis | · ~.invis | 💣 ~.center | · ~.sp .invis | 🍎 ~.center | · ~.invis | · ~.invis | · ~.invis | 👾💣 ~.center |

```

그림과 같이 agent의 action에 따라 agent가 처하는 상황이 달라진다. 1장에서 말했듯이, 이 상황을 state라고 한다. MDP에서는 agent의 행동에 따라 state가 바뀌고, state가 바뀐  곳에서 새로운 action을 취하게 된다. 참고로 그림에서도 알 수 있듯이 지금 문제에서 agent가 취할 수 있는 최선의 action은 왼쪽으로 두 번 이동하는 것이다. 그래야 가장 많은 reward를 얻을 수 있다.
* *\* MDP에서는 '시간'개념이 필요하다. 특정 시간에 agent가 행동을 취하고, 그 결과 새로운 state로 전이한다. 이때의 시간 단위를 time step이라고 한다. time step은 agent가 다음 action을 결정하는 간격이기 때문에 어떤 문제를 풀려는지에 따라 달라진다.

이번에는 다음 그림의 문제를 생각해보자.
```sheet
{
  classes: {
    center: { textAlign: "center", verticalAlign: "middle", padding: "15px" },
    noBorder: { border: "none", backgroundColor: "transparent" },
    redArrow: { color: "red", fontWeight: "bold", fontSize: "22px", textAlign: "right", paddingRight: "0px" },
    c: {backgroundColor: "#aaa"}
  }
}
---
|-| +1 ~.center .noBorder | ~.noBorder   |  ~.center .noBorder | -2 ~.center .noBorder  | +6 ~.center .noBorder |
|-| :-------------------: | :----------: | :-----------------------: | :----------: | :-------------------: |
|- ~.redArrow| 🍎 ~.center .c        |  ~.c            | 👾 ~.center .c .redArrow             |   💣  ~.center .c       | 🍎🍎🍎🍎🍎🍎 ~.center .c          |

```
이 그림에서 grid의 왼쪽 끝에는 reward가 +1인 사과가 있고, agent의 바로 오른쪽에는 reward가 -2인 폭탄이 있다. 그리고 그 너머 오른쪽 끝에는 reward가 +6인 사과 더미가 있다. 이 문제에서 오른쪽으로 이동하면 즉시 얻는 reward는 마이너스이지만, 한 번 더 오른쪽으로 이동하면 사과 더미를 얻을 수 있다. 따라서 이 문제에서 최선의 행동은 오른쪽으로 두 번 이동하는 것이다. (참고로, 이 agent는 한 방향으로만 이동할 수 있기 때문에 왼쪽으로 두 번, 오른쪽으로 네 번 이동해 모든 reward를 받는다는 선택지는 없다고 가정한다.)

이 예제에서 알 수 있듯이 agent는 눈앞의 reward가 아니라 미래에 얻을 수 있는 reward의 총합을 고려해야 한다. 즉, culmulative reward를 maximeze하려 노력해야 한다.

## 2.1.2 interactions between agent and environment
MDP에서는 agent와 environment간의 interaction이 일어난다. 이 때 명심해야 할 사실은 agnet가 action을 취함으로서 state가 변화한다는 점이다. 그에 따라 얻을 수 있는 reward도 달라진다. 
![[Pasted image 20250929201256.png]]
위에서 보듯 시간 t에서의 state가 $S_t$이다 이 상태 $S_t$에서 시작하여 agent가 action $A_t$를 수행하여 reward $R_t$를 얻고, 다음 state인 $S_{t+1}$로 전환된다. 이러한 agent와 environment의 interaction은 다음과 같은 전이를 만들어낸다.
$$
S_{0}, A_{0}, R_{0}, S_{1}, A_{1}, R_{1}, S_{2}, A_{2}, R_{2}, \cdots
$$
이 시계열 데이터는 첫 번째 state를 $S_0$에서 시작한다. state $S_0$에서 agent가 action $A_0$을 수행하여 reward $R_0$를 얻고, 한 step만큼 시간이 흘러 state가 $S_1$로 변한다. 다음 상태인 $S_1$에서 agent가 action $A_1$을 수행하여 reward $R_1$을 얻고, 다음 state인 $S_2$로 변하는 흐름이 계속된다.
* *\* reinforcement learning에서는 다음과 같이 reward의 시점을 $R_t$로 잡기도 하고 $R_{t+1}$로 잡기도 한다.
	 - *state $S_t$에서 action $A_t$를 수행하고 reward $R_t$를 받고 다음 state인 $S_{t+1}$로 전환
	 - *state $S_t$에서 action $A_t$를 수행하고 reward $R_{t+1}$를 받고 다음 state인 $S_{t+1}$로 전환

* *이처럼 reward의 시점을 $R_t$로 잡기도 하고 $R_{t+1}$로 잡기도 한다. 이 글에서는 프로그래밍하기에 더 편리한 첫 번째 방식을 택한다.

이상으로 MDP의 기초를 익혔다.

# 2.2 agent와 environment를 수식으로
MDP는 agent와 environment를 수식으로 표현한다. 그러기 위해서는 다음의 세 요소를 수식으로 표현해야 한다.
- **상태 전이**: 보상은 state는 어떻게 전이되는가?
- **보상**: reward는 어떻게 주어지는가?
- **정책**: agent는 action을 어떻게 결정하는가?
이 세 요소를 모두 수식으로 표현한다면 MDP를 공식으로 표현했다고 볼 수 있다. '상태 전이'부터 살펴보겠다.

## 2.2.1 상태 전이(state transition)
다음 그림을 살펴보겠다.
```sheet
{
  classes: {
    center: { textAlign: "center", verticalAlign: "middle", padding: "14px" },
    sp: { border: "none", backgroundColor: "transparent", minWidth: "80px" },
    noBorder: { border: "none", backgroundColor: "transparent" },
    invis: { color: "transparent" },
    c: { backgroundColor: "#aaa" },
    bigArrow: { fontSize: "30px", fontWeight: "bold"},
    smallProb: { fontSize: "12px", fontWeight: "bold"},
    vCloser: { verticalAlign: "bottom", paddingTop: "2px", paddingBottom: "0px" },
    redArrow: { color: "red", fontWeight: "bold", fontSize: "22px", textAlign: "right", paddingRight: "0px" },
    bold: { fontWeight: "bold" },
    l: {textAlign: "left"},
    r: {textAlign: "right"},
    d: {verticalAlign: "bottom"},
    u: {verticalAlign: "top"}
  }
}
---
|-| · ~.c |⟸ 👾 ~.redArrow .center .c | · ~.invis .c | · ~.invis .noBorder | · ~.invis .noBorder | · ~.invis .noBorder | · ~.c |⟸ 👾 ~.redArrow .center .c | · ~.invis .c | · ~.invis .noBorder | · ~.invis .noBorder |
|-| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|- ~.noBorder | · ~.invis | ↓ ~.center .bigArrow | · ~.invis | · ~.invis | · ~.invis | 0.9 ~.smallProb .r .d | ↙️ ~.bigArrow ~.l | · ~.invis | ↘️ ~.bigArrow .r | 0.1 ~.smallProb .l .d | · ~.invis |
|- ~.redArrow| 👾 ~.center .c | · ~.invis .c | · ~.invis .c | · ~.invis .noBorder | 👾 ~.center .c | · ~.invis .c | · ~.invis .c | · ~.invis .noBorder | · ~.invis .c | 👾 ~.center .c | · ~.invis .c |

```

왼쪽 그림은 agent가 왼쪽으로 이동하려는 action을 선택했고, 그 결과로 agent가 '반드시' 왼쪽으로 이동하는 모습을 보여준다. 이러한 성질을 determinisctic(결정적)이라고 한다. 상태 전이가 deterministic할 경우 다음 상태 $s'$은 현재 state $s$와 action $a$에 의해 '단 하나로' 결정된다. 따라서 함수로는 다음과 같이 표현할 수 있다.
$$
s'=f(s, a)
$$
$f(s,a)$는 state $s$와 action $a$를 입력하면 다음 state $s'$을 출력하는 함수이다. 이 함수를 가리켜 state transition function(상태 전이 함수)라고 한다. 

반면, 오른쪽 그림은 이동을 stochastic(확률적)으로 표현하고 있다. agent가 왼쪽으로 이동하는 action을 선택하더라도 0.9의 probablity로만 왼쪽으로 이동하고, 0.1의 probablity로는 그 자리에 머물러 있는다. 이처럼 행동이 stochastic으로 변하는 이유는 무엇일까? 예를 들어 바닥이 미끄러워일 수도 있고, 내부 메커니즘(모터 등)이 제대로 작동하지 않아서일 수도 있다.
* *\* state transition가 deterministic하더라도 stochastic으로 설명할 수 있다. 그림의 예라면 'agent가 왼쪽으로 이동하는 action을 선택하면 1.0의 probablity로 왼쪽으로 이동한다'라고 기술하면 된다.

이제 state transition을을 표기하는 법을 살펴보겠다. agent가 state $s$에서 action $a$를 선택한다고 해보자. 이 경우 다음 state $s'$으로 이동할 확률은 다음처럼 나타낸다.
$$
p(s'|s,a)
$$
기호 |의 오른쪽에는 '조건'을 나타내는 확률 변수를 적는다. 지금 식에서는 'state $s$에서 action $a$를 선택했다'라는 것이 조건에 해당된다. 이 두 조건이 주어졌을 때 $s'$로 전이할 확률을 $p(s'|s,a)$로 나타내며, 이때 $p(s'|s,a)$를 state transition probablity라고 한다. 다음 그림은 $p(s'|s,a)$의 구체적인 예이다.
```sheet
{
  classes: {
    center: { textAlign: "center", verticalAlign: "middle", padding: "14px" },
    sp: { border: "none", backgroundColor: "transparent", minWidth: "80px" },
    noBorder: { border: "none", backgroundColor: "transparent" },
    invis: { color: "transparent" },
    bigArrow: { fontSize: "30px", fontWeight: "bold"},
    smallProb: { fontSize: "12px", fontWeight: "bold"},
    vCloser: { verticalAlign: "bottom", paddingTop: "2px", paddingBottom: "0px" },
    redArrow: { color: "red", fontWeight: "bold", fontSize: "22px", textAlign: "right", paddingRight: "0px" },
    bold: { fontWeight: "bold" },
    c1: { backgroundColor: "#aaa" },
    c2: { backgroundColor: "#444" },
    l: {textAlign: "left"},
    r: {textAlign: "right"},
    d: {verticalAlign: "bottom"},
    u: {verticalAlign: "top", paddingTop: "0px"}
  }
}
---
|-| L1 ~.center .noBorder | L2 ~.center .noBorder | L3 ~.center .noBorder | L4 ~.center .noBorder | L5 ~.center .noBorder | · ~.invis .noBorder .center | $s'$ ~.center .c2 | L1 ~.center .c2 | L2 ~.center .c2 | L3 ~.center .c2 | L4 ~.center .c2 | L5 ~.center .c2 |
|-| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|- ~.center| · ~.invis .c1 | · ~.invis .c1 | ⟸ 👾 ~.redArrow .center .c1 | · ~.invis .c1 |  · ~.invis .c1 |  · ~.invis .noBorder | $p(s'\| s,a)$ | 0 | 0.9 | 0.1 | 0 | 0 |
```

그림과 같이 총 5개의 칸을 왼쪽부터 순서대로 L1, L2, L3, L4, L5라고 부르기로 하자. 또한 agent의 action에 대해서는 '왼쪽으로 이동'을 Left, '오른쪽으로 이동'을 Right로 표기한다. 이제 agent가 L3에 있을 때, 즉 state가 L3일 때 action으로 Left를 선택했다고 해보자. 그리고 이 경우의 state transition probablity $p(s'|s=L3,a=Left)$가 오른쪽 표와 같다고 하자.
* *\* 그림의 표는 정확히 말하면 상태 전이의 'probablity distribution'이다. 확률 변수 $s$가 취할 수 있는 '모든'값 각각의 확률을 나타냈기 때문이다.

$p(s'|s,a)$가 다음 상태 $s'$을 결정하는 데는 '현재' state $s$와 action $a$만이 영향을 준다. 다시 말해 state sransition에서는 과거의 정보, 즉 지금까지 어떤 state들을 거쳐 왔고 어떤 action을 취해 왔는지는 신경 쓰지 않는다. 이처럼 현재의 정보만 고려하는 성질을 marcov property라고 한다. MDP는 marcov property를 만족한다고 가정하고 state transition(and reward)를 모델링한다. marcov property를 도입하는 가장 큰 이유는 문제를 더 쉽게 풀기 위해서이다. 만약 marcov property를 따른다고 가정하지 않는다면 과거의 모든 상태와 행동까지 고려해야 해서, 그 조합이 기하급수적으로 많아진다.

## 2.2.2 보상 함수(reward function)
다음으로 reward에 대해 생각해보자. 설명이 복잡해지지 않도록 reward는 deterministic하게 주어진다고 가정하자. agent가 state $s$에서 action $a$를 수행하여 다음 state $s'$가 되었을 때의 reward를 $r(s, a, s')$라는 함수로 정의한다. 그럼 reward function의 예로 아래 그림을 보자.
```sheet
{
  classes: {
    center: { textAlign: "center", verticalAlign: "middle", padding: "14px" },
    sp: { border: "none", backgroundColor: "transparent", minWidth: "80px" },
    noBorder: { border: "none", backgroundColor: "transparent" },
    invis: { color: "transparent" },
    bigArrow: { fontSize: "20px", fontWeight: "bold"},
    smallProb: { fontSize: "12px", fontWeight: "bold"},
    vCloser: { verticalAlign: "bottom", paddingTop: "2px", paddingBottom: "0px" },
    redArrow: { color: "red", fontWeight: "bold", fontSize: "22px", textAlign: "right", paddingRight: "0px" },
    bold: { fontWeight: "bold" },
    c1: { backgroundColor: "#aaa" },
    c2: { backgroundColor: "#444" },
    l: {textAlign: "left"},
    r: {textAlign: "right"},
    d: {verticalAlign: "bottom"},
    u: {verticalAlign: "top", paddingTop: "0px"}
  }
}
---
|-| L1 ~.center .noBorder | L2 ~.center .noBorder | L3 ~.center .noBorder | L4 ~.center .noBorder | L5 ~.center .noBorder | · ~.invis .noBorder .center |
|-| :--: | :--: | :--: | :--: | :--: | :--: |
|- ~.center|🍎 ~.redArrow .c1 | ⟸ 👾 ~.redArrow .c1 | · ~.invis .c1 | · ~.invis .c1 |  · ~.invis .c1 |  $r(s=L2,a=Left,s'=L1)=1$ ~.bigArrow .noBorder |
```

그림에서는 agent가 state L2에서(s=L2), action Left를 선택하여 state L1로 전이된 예가 그려져 있다. 이 경우의 reward는 $r(s,a,s')$를 통해 1임을 알 수 있다.
참고로 이 예에서는 다음 state $s'$만 알면 보상이 결정된다. 이번 문제에서는 이동한 위치에 사과가 있느냐에 의해서만 reward가 결정되기 때문이다. 따라서 reward function을 $r(s')$로 구현할 수 있다.
* *\* reward가 stochastic하게 주어질 수도 있다. 예를 들어 어떤 장소에 가면 0.8의 probablity로 적에게 습격을 당해 -10의 reward를 받는 경우이다. 물론 rewardfunction $r(s,a,s')$가 'Expectation of reward'를 반환하도록 왼쪽과 같이 각 state에 따라 '반드시'정해진 action을 한다.

## 2.2.3 에이전트의 정책(policy of agent)
다음으로 agent의 policy에 대해 생각해보자. policy는 agent가 action을 결정하는 방식이다. policy에서 중요한 점은 agent가 '현재 state'만으로 action을 결정할 수 있다는 것이다. '현재 state'만으로 충분한 이유는 무엇일까? 바로 environment의 state transition이 marcov property에 따라 이루어지기 때문이다. environment의 state transition에서는 현재 state $s$와 action $a$만을 고려하여 다음 state $s'$가 졀정된다. 이상이 의미하는 바는 'environment에 대해 필요한 정보는 모두 현재 environment에 있다'는 것이다. 따라서 agent가 '현재 state'만으로 action을 결정할 수 있다.
* *\* MDP의 marcov  property라는 특성은 agent에 대한 제약이 아니라 'environment에 대한 제약'으로 볼 수 있다. 즉, marcov property를 만족하도록 'state'를 관리하는 책임이 environment 쪽에 있다는 뜻이다. agent 관점에서 보면 최선의 선택에 필요한 정보가 모두 현재 state에 담겨 있기 때문에 현재만을 바라보고 행동할 수 있다.

agent는 현재 state를 보고 action을 결정한다. 이때 action을 결정하는 방식인 policy는 'deterministic' 또는 'stochastic'인 경우로 나눌 수 있다. deterministic policy는 다음 그림의 왼쪽과 같이 각 state에 따라 '반드시' 정해진 action을 한다.
```sheet
{
  classes: {
    center: { textAlign: "center", verticalAlign: "middle", padding: "14px" },
    sp: { border: "none", backgroundColor: "transparent", minWidth: "80px" },
    noBorder: { border: "none", backgroundColor: "transparent" },
    invis: { color: "transparent" },
    bigArrow: { fontSize: "30px", fontWeight: "bold"},
    smallProb: { fontSize: "12px", fontWeight: "bold"},
    vCloser: { verticalAlign: "bottom", paddingTop: "2px", paddingBottom: "0px" },
    redArrow: { color: "red", fontWeight: "bold", fontSize: "22px", textAlign: "right", paddingRight: "0px" },
    bold: { fontWeight: "bold" },
    c1: { backgroundColor: "#aaa" },
    c2: { backgroundColor: "#444" },
    l: {textAlign: "left"},
    r: {textAlign: "right"},
    d: {verticalAlign: "bottom"},
    u: {verticalAlign: "top", paddingTop: "0px"}
  }
}
---
|-| L1 ~.center .noBorder | L2 ~.center .noBorder | L3 ~.center .noBorder | L4 ~.center .noBorder | L5 ~.center .noBorder | · ~.invis .noBorder .center | L1 ~.center .noBorder | L2 ~.center .noBorder | L3 ~.center .noBorder | L4 ~.center .noBorder | L5 ~.center .noBorder |
|-| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|- ~.center| · ~.invis .c1 | · ~.invis .c1 | ⟸ 👾 ~.redArrow .center .c1 | · ~.invis .c1 |  · ~.invis .c1 |  · ~.invis .noBorder | · ~.invis .c1 | · ~.invis .c1 | ⟸ 👾 ⟹ ~.redArrow .center .c1 | · ~.invis .c1 |  · ~.invis .c1 |
|- ~.center| · ~.invis .noBorder .center | · ~.invis .noBorder .center | Left | · ~.invis .noBorder .center | · ~.invis .noBorder .center | · ~.invis .noBorder .center | · ~.invis .noBorder .center | · ~.invis .noBorder .center | Left(0.4)   Right(0.6) ~.noBorder .center | · ~.invis .noBorder .center | · ~.invis .noBorder .center |
```

왼쪽 그림에서 agent는 L3에 있을 때는 반드시 왼쪽으로 이동한다. 이러한 deterministic policy는 함수로 다음과 같이 정의할 수 있다.
$$
\alpha=\mu(s)
$$
$\mu(s)$는 매개변수로 state를 건네주면 action $a$를 반환하는 함수이다($\mu$는 '뮤'라고 발음한다). 이 예에서 $\mu(s=L3)$는 Left는 반환한다.

오른쪽 그림은 stochastic policy의 예이다. agent가 왼쪽으로 이동할 probablity는 0.4이고, 오른쪽으로 이동할 probablity는 0.6이다. 이렇게 agent의 action이 stochastic으로 결정되는 policy는 수식으로 다음과 같이 표현할 수 있다.
$$
\pi(a|s)
$$
$\pi(a|s)$는 state $s$에서 action $a$를 취할 probablity를 나타낸다. 그림의 예를 수식으로 표현하면 다음과 같다.
$$
\pi(a=Left|s=L3)=0.4 \\
$$
$$
\pi(a=Right|s=L3)=0.6
$$
* *\* deterministic policy는 stochastic policy로도 표현할 수 있다. 예를 들어 deterministic policy인 $\alpha=\mu(s)$는 state $s$에서 action $a$를 수행할 probablity가 1.0인 probablity distribution으로 나타낼 수 있다.

state transition, reward function, policy를 수식으로 나타내보았다. 다음 절에서는 이 세 수식을 이용해 MDP의 목표를 정의해보겠다.

# 2.3 MDP의 목표
지금까지는 environment와 agent의 action을 수식으로 표현했다. 간단히 복습하면, agent는 policy $\pi(a|s)$에 따라 action을 취한다. 그 action과 state transition probablity $p(s'|s, a)$에 의해 다음 state가 결정된다. 그리고 reward는 reward function $r(s, a, s')$가 결정한다. 이 틀 안에서 optimal policy를 찾는 것이 MDP의 목표이다. optimal policy란 return이 최대가 되는 policy이다(return에 대해서는 2.3.2절 참고).
* *\* agent가 deterministic policy를 따른다면 $\mu(s)$함수로 나타낼 수 있다. 하지만 deterministic policy는 stochastic policy로도 표현될 수 있으니 여기서는 stochastic policy $\pi(a|s)$를 가정하고 진행하겠다. 같은 이유도 environment의 state transision도 stochastic이라고 가정한다.

이번 절에서는 optimal policy를 수식으로 정확하게 표현하고자 한다. 그러려면 먼저 MDP의 문제가 크게 두 가지로 나뉜다는 사실을 알아야 한다. 바로 'episodic task'와 'continuous task'이다.

## 2.3.1 episodic task와 continuous task
MDP는 문제에 따라 episodic task와 continuous task로 나뉜다. episodic task는 '끝'이 있는 문제이다. 예를 들어 바둑은 episodic task로 분류된다. 바둑은 결국 승리/패배/무승부 중 하나로 귀결된다. 그리고 다음 대국은 돌이 하나도 놓여 있지 않은 초기 상태에서 새로 시작한다. episodic task에서는 시작부터 끝까지의 일련의 시도를 episode라고 한다.

다음 그림과 같은 문제도 episodic task이다.
```sheet
{
  classes: {
    center: { textAlign: "center", verticalAlign: "middle", padding: "14px" },
    sp: { border: "none", backgroundColor: "transparent", minWidth: "80px" },
    noBorder: { border: "none", backgroundColor: "transparent" },
    invis: { color: "transparent" },
    bigArrow: { fontSize: "40px", fontWeight: "bold"},
    smallProb: { fontSize: "12px", fontWeight: "bold"},
    vCloser: { verticalAlign: "bottom", paddingTop: "2px", paddingBottom: "0px" },
    redArrow: { color: "red", fontWeight: "bold", fontSize: "22px", textAlign: "right", paddingRight: "0px" },
    bold: { fontWeight: "bold" },
    c1: { backgroundColor: "#aaa" },
    c2: { backgroundColor: "#444" },
    l: {textAlign: "left"},
    r: {textAlign: "right"},
    d: {verticalAlign: "bottom"},
    u: {verticalAlign: "top", paddingTop: "0px"}
  }
}
---
|- ~.noBorder| Start ~.center | · ~.invis | · ~.invis | · ~.invis | Goal ~.center | ⤺ ~.bigArrow .center | Start ~.center | · ~.invis | · ~.invis | · ~.invis | Goal ~.center |
|-| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|- ~.center| 👾 ~.redArrow .center .c1 | · ~.invis .c1 | · ~.invis .c1 | · ~.invis .c1 |  · ~.invis .c1 | ⇢ ~.center .bigArrow .noBorder | · ~.invis .c1 | · ~.invis .c1 | · ~.invis .c1 |  · ~.invis .c1 | 👾🍎 ~.redArrow .center .c1 |

```

이 그림에서는 어느 곳엔가 목표가 있고 그 목표에 도달하면 끝이 난다. 그 후에는 다시 초기 state에서 새로운 episode가 시작된다.

반면 continuous task는 '끝'이 없는 문제이다. 예를 들면 재고 관리가 있다. 재고 관리에서의 agent는 얼마나 많은 상품을 구매할지를 결정한다. 판매량과 재고량을 보고 최적의 구매량을 결정해야 한다(재고가 너무 많아지지 않게 관리하면서도 상품 판매에 지장이 없도록 해야 한다). 이런 유형의 문제는 끝을 정하지 않고 영원히 지속될 수 있다.

## 2.3.2 return
다음으로 새로운 용어인 return을 소개하겠다. 이 return을 극대화하는 것이 agent의 목표이다. 
시간 $t$에서의 state를 $S_t$라고 해보자($t$는 임의의 값). 그리고 agent가 정책 $\pi$에 따라 action $A_t$를 하고, reward $R_t$를 얻고, 새로운 state $S_{t+1}$로 전이하는 흐름이 이어진다. 이때 return $G_t$는 다음과 같이 정해진다.
$$
G_{t}=R_{t}+\gamma R_{t+1}+\gamma^2R_{t+2}+\cdots
$$
위의 식과 같이 return은 agent가 얻는 reward의 합이다. 하지만 시간이 지날수록 reward는 $\gamma$(감마)에 의해 기하급수적으로 줄어든다. 이 $\gamma$를 discount rate라고 하며, 0.0에서 1.0사이 실수로 정의된다. 예를 들어 discount rate를 0.9로 설정하면 식은 다음과 같이 변한다.
$$
G_{t}=R_{t}+0.9 R_{t+1}+0.81R_{t+2}+\cdots
$$
discount rate를 도입하는 가장 주된 이유는 continuous task에서 return이 무한ㄷ가 되지 않도록 방지하기 위해서이다. discount rate가 없다면, 즉 $\gamma=1$이라면 continuous task에서 return이 무한대까지 늘어나게 된다. 

또한 discount rate는 가까운 미래의 reward를 더 중요하게 보이도록 한다. 이 특성은 사람, 나아가 생물의 많은 행동 원리를 설명할 수 있다. 예를 들어 오늘 1만원 받기와 10년 후에 2만원 받기 중 어느 것을 선택할 것인가? discount rate때문에 미래 보상이 기하급수적으로 줄어든다면 즉시 얻을 수 있는 reward에 더 큰 매력을 느낄 것이다.

## 2.3.3 state-value function
방금 정의한 'return'을 극대화하는 것이 agent의 목표라고 하였다. 여기서 주의할 게 하나 있다. agent와 environment가 'stochastic'하게 동작할 수 있다는 점이다. agent는 다음 action을 stochastic하게 결정할 수 있고, state 역시 stochastic하게 전이될 수 있다. 그렇다면 얻는 return 역시 'stochastic'하게 달라질 것이다. 예를 들어 어떤 episode엥서는 return이 10.4이고 다른 episode에서는 return이 8.7인 식으로 말이다. 비록 같은 state에서 시작하더라도 state가 episode마다 stochastic하게 달라질 수 있다.

이러한 확률적 동작에 대응하기 위해서는 Expectation, 즉 'Expectation of return'을 지표로 삼아야 한다. 상태 $S_t$가 $s$이고($t$는 임의의 값), agent의 policy가 $\pi$일 때, agent가 얻을 수 있는 기대 수익을 다음과 같이 표현할 수 있다.
$$
v_{\pi}(s)=\mathbb{E}[G_{t}|S_{t}=s, \pi]
$$
이처럼 return의 Expectation을 $v_{\pi}(s)$로 표기하며 $v_{\pi}(s)$를 state-value function이라고 한다. 위 식의 우변에서 agent의 policy $\pi$는 조건으로 주어진다. ploicy $\pi$가 바뀌면 agent가 얻는 reward도 바뀌고 총합인 return도 바뀌기 때문이다. 이런 특성을 명시하기 위해 state-value function은 $v_{\pi}(s)$처럼 $\pi$를 $v$의 밑으로 쓰는 방식을 많이 따른다. 또한 위 식을 다음과 같이 쓰기도 한다.
$$
v_{\pi}(s)=\mathbb{E}_{\pi}[G_{t}|S_{t}=s]
$$
위의 식에서는 우변에서 $\pi$의 위치가 $\mathbb{E}_{\pi}$로 옮겨갔다. 의미는 원래 식과 마찬가지로 $\pi$라는 policy가 조건으로 주어져 있음을 나타낸다. 이 글에서는 이 형태로 사용하겠다.
* *\* 



```
{
  classes: {
    center: { textAlign: "center", verticalAlign: "middle", padding: "14px" },
    sp: { border: "none", backgroundColor: "transparent", minWidth: "80px" },
    noBorder: { border: "none", backgroundColor: "transparent" },
    invis: { color: "transparent" },
    c: { backgroundColor: "#aaa" },
    big: { fontSize: "30px", fontWeight: "bold"},
    smallProb: { fontSize: "12px", fontWeight: "bold"},
    redArrow: { color: "red", fontWeight: "bold", fontSize: "22px", textAlign: "right", paddingRight: "0px" },
    bold: { fontWeight: "bold" },
    l: {textAlign: "left"},
    r: {textAlign: "right"},
    d: {verticalAlign: "bottom"},
    u: {verticalAlign: "top"}
  }
}
---
| bandit ~ .vMid   | - ~ .c | reward      | < | < |
| ^                | - ~ .c | 1st | 2nd | 3rd |
| :----: ~ .padX_1 .c | - | :----: ~ .padX_2 | :----: ~ .padX_2 | :----: ~ .padX_2 |
| a                | - | 0  | 1 | 5 |
| b                | - | 1  | 0 | 0 |

| · ~.invis |
🍎
⟸ 👾 ~.redArrow .center .c
```
