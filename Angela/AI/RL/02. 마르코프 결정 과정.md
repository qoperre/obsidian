bandit problemì—ì„œëŠ” agentê°€ ì–´ë–¤ actionì„ ì·¨í•˜ë“  environmentì˜ ì„¤ì •ì€ ë³€í•˜ì§€ ì•ŠìŒ. agentëŠ” ì„¤ì •ì„ ë°”ê¿€ ìˆ˜ ì—†ëŠ” bnaditì„ ë°˜ë³µí•´ì„œ í”Œë ˆì´í•˜ë©´ì„œ ê·¸ ì¤‘ ê°€ì¥ valueê°€ ë†’ì€ banditì„ ì°¾ëŠ”ë‹¤. ê·¸ëŸ¬ë‚˜ ë°”ë‘‘ê³¼ ê°™ì€ ë¬¸ì œì˜ ê²½ìš°, ì–´ë–¤ ìˆ˜ë¥¼ ë‘ë©´ ë°”ë‘‘íŒ ìœ„ì˜ ëŒ ë°°ì¹˜(environment)ê°€ ë‹¬ë¼ì§„ë‹¤. ê·¸ë¦¬ê³  ìƒëŒ€ê°€ ëŒì„ ë‘ë©´ ë˜ ë‹¬ë¼ì§„ë‹¤. ì´ì²˜ëŸ¼ agentì˜ actionì— ë”°ë¼ envirionmentê°€ ë³€í•˜ëŠ” ë¬¸ì œë“¤ì´ ëŒ€ë¶€ë¶„ì´ê³ , ë”°ë¼ì„œ agentëŠ” envirionmentê°€ ë³€í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì—¬ ìµœì„ ì˜ ìˆ˜ë¥¼ ë‘ì–´ì•¼ í•œë‹¤.
* *\*1ì¥ì—ì„œëŠ” banditì˜ reward ì„¤ì •(rewardì˜ í™•ë¥  ë¶„í¬)ì´ ì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ” non-stational plroblemë„ ìˆì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ non-stational problemì—ì„œì˜ rewardì˜ 'probablity distribution'ì€ (agentê°€ ì–´ë–¤ actionì„ ì·¨í•˜ëŠ”ì§€ì™€ëŠ” ê´€ê³„ ì—†ì´)ì‹œê°„ì— ë”°ë¼ì„œë§Œ ë³€í™”í•œë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë²ˆ ì¥ì—ì„œëŠ” 'agentì˜ actionì— ë”°ë¼'envirionmentì˜ stateê°€ ë³€í•˜ëŠ” ë¬¸ì œë¥¼ ë‹¤ë£¬ë‹¤.

ì´ëŸ¬í•œ ë¬¸ì œì˜ ëŒ€í‘œì ì¸ ì˜ˆë¡œ Marcov Decision Process(MDP)ì´ ìˆë‹¤. ë¨¼ì € MDPì˜ ëª©í‘œë¥¼ ëª…í™•íˆ ê·œì •í•œ í›„, ê°„ë‹¨í•œ MDP ë¬¸ì œë¥¼ í’€ì–´ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” ê³¼ì •ì„ ì‚´í´ë³´ì.

# 2.1 MDPë€?

MDPì—ì„œ Decision Problemì´ë€ 'agentê°€(environmentì™€ interactí•˜ë©´ì„œ) actionì„ ì·¨í•˜ëŠ” ê³¼ì •'ì„ ëœ»í•œë‹¤.

## 2.1.1 Example
ì•„ë˜ ê·¸ë¦¼ì„ ë³´ë©´ ì„¸ìƒì€ gridë¡œ êµ¬ë¶„ë˜ì–´ ìˆê³ , ê·¸ ì•ˆì— ë¡œë´‡(agent)ê°€ ìˆë‹¤. agentëŠ” ì˜¤ë¥¸ìª½ ë˜ëŠ” ì™¼ìª½ìœ¼ë¡œ ì´ë™í•  ìˆ˜ ìˆë‹¤. ì´ ê¸€ì—ì„œëŠ” ì´ì™€ ê°™ì€ ì„¸ê³„ë¥¼ 'grid world'ë¼ê³  ë¶€ë¥´ê¸°ë¡œ í•˜ê² ë‹¤.
```sheet
{
  classes: {
    center: { textAlign: "center", verticalAlign: "middle", padding: "15px" },
    noBorder: { border: "none", backgroundColor: "transparent" },
    redArrow: { color: "red", fontWeight: "bold", fontSize: "22px", textAlign: "right", paddingRight: "0px" },
    c: {backgroundColor: "#aaa"}
  }
}
---
|-| +1 ~.center .noBorder | ~.noBorder   | â† â— â— â†’ ~.center .noBorder |  ~.noBorder  | -2 ~.center .noBorder |
|-| :-------------------: | :----------: | :-----------------------: | :----------: | :-------------------: |
|- ~.redArrow| ğŸ ~.center .c        |  ~.c            | ğŸ‘¾ ~.center .c  .redArrow            |     ~.c       | ğŸ’£ ~.center .c          |

```
ë¡œë´‡ì´ agentì´ê³ , ì£¼ë³€ì´ environmentì´ë‹¤. agentëŠ” ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™í•˜ê±°ë‚˜ ì™¼ìª½ìœ¼ë¡œ ì´ë™í•˜ëŠ” ë‘ ê°€ì§€ í–‰ë™ì„ ì·¨í•  ìˆ˜ ìˆë‹¤. ë˜í•œ ê·¸ë¦¼ì—ì„œì˜ ê°€ì¥ ì™¼ìª½ ì¹¸ì—ëŠ” ì‚¬ê³¼ê°€ ìˆê³  ì˜¤ë¥¸ìª½ ì¹¸ì—ëŠ” í­íƒ„ì´ ìˆë‹¤. ì´ë“¤ì€ agentì—ê²Œ ì£¼ì–´ì§€ëŠ” rewardì´ë‹¤. ì‚¬ê³¼ë¥¼ ì–»ì„ ë•Œì˜ rewardëŠ” +1ë¡œ, í­íƒ„ì„ ì–»ì„ ë•Œì˜ rewardëŠ” -2ë¡œ í•˜ê² ë‹¤. ë¹ˆì¹¸ì˜ rewardëŠ” 0ì´ë‹¤.

ì´ ë¬¸ì œì—ì„œëŠ” agentê°€ í–‰ë™í•  ë•Œë§ˆë‹¤ ìƒí™©ì´ ë³€í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì™¼ìª½ìœ¼ë¡œ ë‘ ë²ˆ ì—°ë‹¬ì•„ ì´ë™í–ˆë‹¤ê³  ê°€ì •í•´ë³´ì. ê·¸ëŸ¬ë©´ agentëŠ” +1ì˜ rewardë¥¼ ì–»ëŠ”ë‹¤. ë°˜ëŒ€ë¡œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ë‘ ë²ˆ ì´ë™í•˜ë©´ -2ì˜ rewardë¥¼ ë°›ëŠ”ë‹¤. ì´ëŸ¬í•œ agentì˜ ì´ë™ì„ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

```sheet
{
  classes: {
    center: { textAlign: "center", verticalAlign: "middle", padding: "14px" },
    sp: { border: "none", backgroundColor: "transparent", minWidth: "80px" },
    noBorder: { border: "none", backgroundColor: "transparent" },
    invis: { color: "transparent" },
    c: { backgroundColor: "#aaa" },
    bigArrow: { fontSize: "30px", fontWeight: "bold" },
    vCloser: { verticalAlign: "bottom", paddingTop: "2px", paddingBottom: "0px" },
    redArrow: { color: "red", fontWeight: "bold", fontSize: "22px", textAlign: "right", paddingRight: "0px" },
    bold: { fontWeight: "bold" }
  }
}
---
|- ~.redArrow| Â· ~.invis .noBorder | Â· ~.invis .noBorder | Â· ~.invis .noBorder | ğŸ ~.center .c | Â· ~.invis .c | ğŸ‘¾ ~.center .c .redArrow | Â· ~.invis .c | ğŸ’£ ~.center .c | Â· ~.invis .noBorder | Â· ~.invis .noBorder | Â· ~.invis .noBorder |
|-| :--: | :--: | :--: | :--: | :--: | :---: ~.sp | :--: | :--: | :--: | :--: | :--: |
|- ~.noBorder| Â· ~.invis | Â· ~.invis | Â· ~.invis | Â· ~.invis | â†™ï¸ ~.center .bigArrow | Â· ~.sp .invis | â†˜ï¸ ~.center .bigArrow | Â· ~.invis | Â· ~.invis | Â· ~.invis | Â· ~.invis |
|- ~.c .redArrow| ğŸ ~.center | ğŸ‘¾ ~.center .redArrow | Â· ~.invis | Â· ~.invis | ğŸ’£ ~.center | Â· ~.sp .invis | ğŸ ~.center | Â· ~.invis | Â· ~.invis | ğŸ‘¾ ~.center .redArrow | ğŸ’£ ~.center |
|- ~.noBorder| +1 ~.center .vCloser .bold | Â· ~.invis | â†“ ~.center .bigArrow | Â· ~.invis | Â· ~.invis | Â· ~.sp .invis | Â· ~.invis | Â· ~.invis | â†“ ~.center .bigArrow | Â· ~.invis | -2 ~.center .vCloser .bold |
|- ~.c .redArrow| ğŸ‘¾ğŸ ~.center | Â· ~.invis | Â· ~.invis | Â· ~.invis | ğŸ’£ ~.center | Â· ~.sp .invis | ğŸ ~.center | Â· ~.invis | Â· ~.invis | Â· ~.invis | ğŸ‘¾ğŸ’£ ~.center |

```

ê·¸ë¦¼ê³¼ ê°™ì´ agentì˜ actionì— ë”°ë¼ agentê°€ ì²˜í•˜ëŠ” ìƒí™©ì´ ë‹¬ë¼ì§„ë‹¤. 1ì¥ì—ì„œ ë§í–ˆë“¯ì´, ì´ ìƒí™©ì„ stateë¼ê³  í•œë‹¤. MDPì—ì„œëŠ” agentì˜ í–‰ë™ì— ë”°ë¼ stateê°€ ë°”ë€Œê³ , stateê°€ ë°”ë€  ê³³ì—ì„œ ìƒˆë¡œìš´ actionì„ ì·¨í•˜ê²Œ ëœë‹¤. ì°¸ê³ ë¡œ ê·¸ë¦¼ì—ì„œë„ ì•Œ ìˆ˜ ìˆë“¯ì´ ì§€ê¸ˆ ë¬¸ì œì—ì„œ agentê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ìµœì„ ì˜ actionì€ ì™¼ìª½ìœ¼ë¡œ ë‘ ë²ˆ ì´ë™í•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì•¼ ê°€ì¥ ë§ì€ rewardë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.
* *\* MDPì—ì„œëŠ” 'ì‹œê°„'ê°œë…ì´ í•„ìš”í•˜ë‹¤. íŠ¹ì • ì‹œê°„ì— agentê°€ í–‰ë™ì„ ì·¨í•˜ê³ , ê·¸ ê²°ê³¼ ìƒˆë¡œìš´ stateë¡œ ì „ì´í•œë‹¤. ì´ë•Œì˜ ì‹œê°„ ë‹¨ìœ„ë¥¼ time stepì´ë¼ê³  í•œë‹¤. time stepì€ agentê°€ ë‹¤ìŒ actionì„ ê²°ì •í•˜ëŠ” ê°„ê²©ì´ê¸° ë•Œë¬¸ì— ì–´ë–¤ ë¬¸ì œë¥¼ í’€ë ¤ëŠ”ì§€ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤.

ì´ë²ˆì—ëŠ” ë‹¤ìŒ ê·¸ë¦¼ì˜ ë¬¸ì œë¥¼ ìƒê°í•´ë³´ì.
```sheet
{
  classes: {
    center: { textAlign: "center", verticalAlign: "middle", padding: "15px" },
    noBorder: { border: "none", backgroundColor: "transparent" },
    redArrow: { color: "red", fontWeight: "bold", fontSize: "22px", textAlign: "right", paddingRight: "0px" },
    c: {backgroundColor: "#aaa"}
  }
}
---
|-| +1 ~.center .noBorder | ~.noBorder   |  ~.center .noBorder | -2 ~.center .noBorder  | +6 ~.center .noBorder |
|-| :-------------------: | :----------: | :-----------------------: | :----------: | :-------------------: |
|- ~.redArrow| ğŸ ~.center .c        |  ~.c            | ğŸ‘¾ ~.center .c .redArrow             |   ğŸ’£  ~.center .c       | ğŸğŸğŸğŸğŸğŸ ~.center .c          |

```
ì´ ê·¸ë¦¼ì—ì„œ gridì˜ ì™¼ìª½ ëì—ëŠ” rewardê°€ +1ì¸ ì‚¬ê³¼ê°€ ìˆê³ , agentì˜ ë°”ë¡œ ì˜¤ë¥¸ìª½ì—ëŠ” rewardê°€ -2ì¸ í­íƒ„ì´ ìˆë‹¤. ê·¸ë¦¬ê³  ê·¸ ë„ˆë¨¸ ì˜¤ë¥¸ìª½ ëì—ëŠ” rewardê°€ +6ì¸ ì‚¬ê³¼ ë”ë¯¸ê°€ ìˆë‹¤. ì´ ë¬¸ì œì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™í•˜ë©´ ì¦‰ì‹œ ì–»ëŠ” rewardëŠ” ë§ˆì´ë„ˆìŠ¤ì´ì§€ë§Œ, í•œ ë²ˆ ë” ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™í•˜ë©´ ì‚¬ê³¼ ë”ë¯¸ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì´ ë¬¸ì œì—ì„œ ìµœì„ ì˜ í–‰ë™ì€ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ë‘ ë²ˆ ì´ë™í•˜ëŠ” ê²ƒì´ë‹¤. (ì°¸ê³ ë¡œ, ì´ agentëŠ” í•œ ë°©í–¥ìœ¼ë¡œë§Œ ì´ë™í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì™¼ìª½ìœ¼ë¡œ ë‘ ë²ˆ, ì˜¤ë¥¸ìª½ìœ¼ë¡œ ë„¤ ë²ˆ ì´ë™í•´ ëª¨ë“  rewardë¥¼ ë°›ëŠ”ë‹¤ëŠ” ì„ íƒì§€ëŠ” ì—†ë‹¤ê³  ê°€ì •í•œë‹¤.)

ì´ ì˜ˆì œì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ agentëŠ” ëˆˆì•ì˜ rewardê°€ ì•„ë‹ˆë¼ ë¯¸ë˜ì— ì–»ì„ ìˆ˜ ìˆëŠ” rewardì˜ ì´í•©ì„ ê³ ë ¤í•´ì•¼ í•œë‹¤. ì¦‰, culmulative rewardë¥¼ maximezeí•˜ë ¤ ë…¸ë ¥í•´ì•¼ í•œë‹¤.

## 2.1.2 interactions between agent and environment
MDPì—ì„œëŠ” agentì™€ environmentê°„ì˜ interactionì´ ì¼ì–´ë‚œë‹¤. ì´ ë•Œ ëª…ì‹¬í•´ì•¼ í•  ì‚¬ì‹¤ì€ agnetê°€ actionì„ ì·¨í•¨ìœ¼ë¡œì„œ stateê°€ ë³€í™”í•œë‹¤ëŠ” ì ì´ë‹¤. ê·¸ì— ë”°ë¼ ì–»ì„ ìˆ˜ ìˆëŠ” rewardë„ ë‹¬ë¼ì§„ë‹¤. 
![[Pasted image 20250929201256.png]]
ìœ„ì—ì„œ ë³´ë“¯ ì‹œê°„ tì—ì„œì˜ stateê°€ $S_t$ì´ë‹¤ ì´ ìƒíƒœ $S_t$ì—ì„œ ì‹œì‘í•˜ì—¬ agentê°€ action $A_t$ë¥¼ ìˆ˜í–‰í•˜ì—¬ reward $R_t$ë¥¼ ì–»ê³ , ë‹¤ìŒ stateì¸ $S_{t+1}$ë¡œ ì „í™˜ëœë‹¤. ì´ëŸ¬í•œ agentì™€ environmentì˜ interactionì€ ë‹¤ìŒê³¼ ê°™ì€ ì „ì´ë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤.
$$
S_{0}, A_{0}, R_{0}, S_{1}, A_{1}, R_{1}, S_{2}, A_{2}, R_{2}, \cdots
$$
ì´ ì‹œê³„ì—´ ë°ì´í„°ëŠ” ì²« ë²ˆì§¸ stateë¥¼ $S_0$ì—ì„œ ì‹œì‘í•œë‹¤. state $S_0$ì—ì„œ agentê°€ action $A_0$ì„ ìˆ˜í–‰í•˜ì—¬ reward $R_0$ë¥¼ ì–»ê³ , í•œ stepë§Œí¼ ì‹œê°„ì´ í˜ëŸ¬ stateê°€ $S_1$ë¡œ ë³€í•œë‹¤. ë‹¤ìŒ ìƒíƒœì¸ $S_1$ì—ì„œ agentê°€ action $A_1$ì„ ìˆ˜í–‰í•˜ì—¬ reward $R_1$ì„ ì–»ê³ , ë‹¤ìŒ stateì¸ $S_2$ë¡œ ë³€í•˜ëŠ” íë¦„ì´ ê³„ì†ëœë‹¤.
* *\* reinforcement learningì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ rewardì˜ ì‹œì ì„ $R_t$ë¡œ ì¡ê¸°ë„ í•˜ê³  $R_{t+1}$ë¡œ ì¡ê¸°ë„ í•œë‹¤.
	 - *state $S_t$ì—ì„œ action $A_t$ë¥¼ ìˆ˜í–‰í•˜ê³  reward $R_t$ë¥¼ ë°›ê³  ë‹¤ìŒ stateì¸ $S_{t+1}$ë¡œ ì „í™˜
	 - *state $S_t$ì—ì„œ action $A_t$ë¥¼ ìˆ˜í–‰í•˜ê³  reward $R_{t+1}$ë¥¼ ë°›ê³  ë‹¤ìŒ stateì¸ $S_{t+1}$ë¡œ ì „í™˜

* *ì´ì²˜ëŸ¼ rewardì˜ ì‹œì ì„ $R_t$ë¡œ ì¡ê¸°ë„ í•˜ê³  $R_{t+1}$ë¡œ ì¡ê¸°ë„ í•œë‹¤. ì´ ê¸€ì—ì„œëŠ” í”„ë¡œê·¸ë˜ë°í•˜ê¸°ì— ë” í¸ë¦¬í•œ ì²« ë²ˆì§¸ ë°©ì‹ì„ íƒí•œë‹¤.

ì´ìƒìœ¼ë¡œ MDPì˜ ê¸°ì´ˆë¥¼ ìµí˜”ë‹¤.

# 2.2 agentì™€ environmentë¥¼ ìˆ˜ì‹ìœ¼ë¡œ
MDPëŠ” agentì™€ environmentë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•œë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒì˜ ì„¸ ìš”ì†Œë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•´ì•¼ í•œë‹¤.
- **ìƒíƒœ ì „ì´**: ë³´ìƒì€ stateëŠ” ì–´ë–»ê²Œ ì „ì´ë˜ëŠ”ê°€?
- **ë³´ìƒ**: rewardëŠ” ì–´ë–»ê²Œ ì£¼ì–´ì§€ëŠ”ê°€?
- **ì •ì±…**: agentëŠ” actionì„ ì–´ë–»ê²Œ ê²°ì •í•˜ëŠ”ê°€?
ì´ ì„¸ ìš”ì†Œë¥¼ ëª¨ë‘ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•œë‹¤ë©´ MDPë¥¼ ê³µì‹ìœ¼ë¡œ í‘œí˜„í–ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. 'ìƒíƒœ ì „ì´'ë¶€í„° ì‚´í´ë³´ê² ë‹¤.

## 2.2.1 ìƒíƒœ ì „ì´(state transition)
ë‹¤ìŒ ê·¸ë¦¼ì„ ì‚´í´ë³´ê² ë‹¤.
```sheet
{
  classes: {
    center: { textAlign: "center", verticalAlign: "middle", padding: "14px" },
    sp: { border: "none", backgroundColor: "transparent", minWidth: "80px" },
    noBorder: { border: "none", backgroundColor: "transparent" },
    invis: { color: "transparent" },
    c: { backgroundColor: "#aaa" },
    bigArrow: { fontSize: "30px", fontWeight: "bold"},
    smallProb: { fontSize: "12px", fontWeight: "bold"},
    vCloser: { verticalAlign: "bottom", paddingTop: "2px", paddingBottom: "0px" },
    redArrow: { color: "red", fontWeight: "bold", fontSize: "22px", textAlign: "right", paddingRight: "0px" },
    bold: { fontWeight: "bold" },
    l: {textAlign: "left"},
    r: {textAlign: "right"},
    d: {verticalAlign: "bottom"},
    u: {verticalAlign: "top"}
  }
}
---
|-| Â· ~.c |âŸ¸ ğŸ‘¾ ~.redArrow .center .c | Â· ~.invis .c | Â· ~.invis .noBorder | Â· ~.invis .noBorder | Â· ~.invis .noBorder | Â· ~.c |âŸ¸ ğŸ‘¾ ~.redArrow .center .c | Â· ~.invis .c | Â· ~.invis .noBorder | Â· ~.invis .noBorder |
|-| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|- ~.noBorder | Â· ~.invis | â†“ ~.center .bigArrow | Â· ~.invis | Â· ~.invis | Â· ~.invis | 0.9 ~.smallProb .r .d | â†™ï¸ ~.bigArrow ~.l | Â· ~.invis | â†˜ï¸ ~.bigArrow .r | 0.1 ~.smallProb .l .d | Â· ~.invis |
|- ~.redArrow| ğŸ‘¾ ~.center .c | Â· ~.invis .c | Â· ~.invis .c | Â· ~.invis .noBorder | ğŸ‘¾ ~.center .c | Â· ~.invis .c | Â· ~.invis .c | Â· ~.invis .noBorder | Â· ~.invis .c | ğŸ‘¾ ~.center .c | Â· ~.invis .c |

```

ì™¼ìª½ ê·¸ë¦¼ì€ agentê°€ ì™¼ìª½ìœ¼ë¡œ ì´ë™í•˜ë ¤ëŠ” actionì„ ì„ íƒí–ˆê³ , ê·¸ ê²°ê³¼ë¡œ agentê°€ 'ë°˜ë“œì‹œ' ì™¼ìª½ìœ¼ë¡œ ì´ë™í•˜ëŠ” ëª¨ìŠµì„ ë³´ì—¬ì¤€ë‹¤. ì´ëŸ¬í•œ ì„±ì§ˆì„ determinisctic(ê²°ì •ì )ì´ë¼ê³  í•œë‹¤. ìƒíƒœ ì „ì´ê°€ deterministicí•  ê²½ìš° ë‹¤ìŒ ìƒíƒœ $s'$ì€ í˜„ì¬ state $s$ì™€ action $a$ì— ì˜í•´ 'ë‹¨ í•˜ë‚˜ë¡œ' ê²°ì •ëœë‹¤. ë”°ë¼ì„œ í•¨ìˆ˜ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.
$$
s'=f(s, a)
$$
$f(s,a)$ëŠ” state $s$ì™€ action $a$ë¥¼ ì…ë ¥í•˜ë©´ ë‹¤ìŒ state $s'$ì„ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤. ì´ í•¨ìˆ˜ë¥¼ ê°€ë¦¬ì¼œ state transition function(ìƒíƒœ ì „ì´ í•¨ìˆ˜)ë¼ê³  í•œë‹¤. 

ë°˜ë©´, ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì€ ì´ë™ì„ stochastic(í™•ë¥ ì )ìœ¼ë¡œ í‘œí˜„í•˜ê³  ìˆë‹¤. agentê°€ ì™¼ìª½ìœ¼ë¡œ ì´ë™í•˜ëŠ” actionì„ ì„ íƒí•˜ë”ë¼ë„ 0.9ì˜ probablityë¡œë§Œ ì™¼ìª½ìœ¼ë¡œ ì´ë™í•˜ê³ , 0.1ì˜ probablityë¡œëŠ” ê·¸ ìë¦¬ì— ë¨¸ë¬¼ëŸ¬ ìˆëŠ”ë‹¤. ì´ì²˜ëŸ¼ í–‰ë™ì´ stochasticìœ¼ë¡œ ë³€í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ? ì˜ˆë¥¼ ë“¤ì–´ ë°”ë‹¥ì´ ë¯¸ë„ëŸ¬ì›Œì¼ ìˆ˜ë„ ìˆê³ , ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜(ëª¨í„° ë“±)ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šì•„ì„œì¼ ìˆ˜ë„ ìˆë‹¤.
* *\* state transitionê°€ deterministicí•˜ë”ë¼ë„ stochasticìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¼ì˜ ì˜ˆë¼ë©´ 'agentê°€ ì™¼ìª½ìœ¼ë¡œ ì´ë™í•˜ëŠ” actionì„ ì„ íƒí•˜ë©´ 1.0ì˜ probablityë¡œ ì™¼ìª½ìœ¼ë¡œ ì´ë™í•œë‹¤'ë¼ê³  ê¸°ìˆ í•˜ë©´ ëœë‹¤.

ì´ì œ state transitionì„ì„ í‘œê¸°í•˜ëŠ” ë²•ì„ ì‚´í´ë³´ê² ë‹¤. agentê°€ state $s$ì—ì„œ action $a$ë¥¼ ì„ íƒí•œë‹¤ê³  í•´ë³´ì. ì´ ê²½ìš° ë‹¤ìŒ state $s'$ìœ¼ë¡œ ì´ë™í•  í™•ë¥ ì€ ë‹¤ìŒì²˜ëŸ¼ ë‚˜íƒ€ë‚¸ë‹¤.
$$
p(s'|s,a)
$$
ê¸°í˜¸ |ì˜ ì˜¤ë¥¸ìª½ì—ëŠ” 'ì¡°ê±´'ì„ ë‚˜íƒ€ë‚´ëŠ” í™•ë¥  ë³€ìˆ˜ë¥¼ ì ëŠ”ë‹¤. ì§€ê¸ˆ ì‹ì—ì„œëŠ” 'state $s$ì—ì„œ action $a$ë¥¼ ì„ íƒí–ˆë‹¤'ë¼ëŠ” ê²ƒì´ ì¡°ê±´ì— í•´ë‹¹ëœë‹¤. ì´ ë‘ ì¡°ê±´ì´ ì£¼ì–´ì¡Œì„ ë•Œ $s'$ë¡œ ì „ì´í•  í™•ë¥ ì„ $p(s'|s,a)$ë¡œ ë‚˜íƒ€ë‚´ë©°, ì´ë•Œ $p(s'|s,a)$ë¥¼ state transition probablityë¼ê³  í•œë‹¤. ë‹¤ìŒ ê·¸ë¦¼ì€ $p(s'|s,a)$ì˜ êµ¬ì²´ì ì¸ ì˜ˆì´ë‹¤.
```sheet
{
  classes: {
    center: { textAlign: "center", verticalAlign: "middle", padding: "14px" },
    sp: { border: "none", backgroundColor: "transparent", minWidth: "80px" },
    noBorder: { border: "none", backgroundColor: "transparent" },
    invis: { color: "transparent" },
    bigArrow: { fontSize: "30px", fontWeight: "bold"},
    smallProb: { fontSize: "12px", fontWeight: "bold"},
    vCloser: { verticalAlign: "bottom", paddingTop: "2px", paddingBottom: "0px" },
    redArrow: { color: "red", fontWeight: "bold", fontSize: "22px", textAlign: "right", paddingRight: "0px" },
    bold: { fontWeight: "bold" },
    c1: { backgroundColor: "#aaa" },
    c2: { backgroundColor: "#444" },
    l: {textAlign: "left"},
    r: {textAlign: "right"},
    d: {verticalAlign: "bottom"},
    u: {verticalAlign: "top", paddingTop: "0px"}
  }
}
---
|-| L1 ~.center .noBorder | L2 ~.center .noBorder | L3 ~.center .noBorder | L4 ~.center .noBorder | L5 ~.center .noBorder | Â· ~.invis .noBorder .center | $s'$ ~.center .c2 | L1 ~.center .c2 | L2 ~.center .c2 | L3 ~.center .c2 | L4 ~.center .c2 | L5 ~.center .c2 |
|-| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|- ~.center| Â· ~.invis .c1 | Â· ~.invis .c1 | âŸ¸ ğŸ‘¾ ~.redArrow .center .c1 | Â· ~.invis .c1 |  Â· ~.invis .c1 |  Â· ~.invis .noBorder | $p(s'\| s,a)$ | 0 | 0.9 | 0.1 | 0 | 0 |
```

ê·¸ë¦¼ê³¼ ê°™ì´ ì´ 5ê°œì˜ ì¹¸ì„ ì™¼ìª½ë¶€í„° ìˆœì„œëŒ€ë¡œ L1, L2, L3, L4, L5ë¼ê³  ë¶€ë¥´ê¸°ë¡œ í•˜ì. ë˜í•œ agentì˜ actionì— ëŒ€í•´ì„œëŠ” 'ì™¼ìª½ìœ¼ë¡œ ì´ë™'ì„ Left, 'ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™'ì„ Rightë¡œ í‘œê¸°í•œë‹¤. ì´ì œ agentê°€ L3ì— ìˆì„ ë•Œ, ì¦‰ stateê°€ L3ì¼ ë•Œ actionìœ¼ë¡œ Leftë¥¼ ì„ íƒí–ˆë‹¤ê³  í•´ë³´ì. ê·¸ë¦¬ê³  ì´ ê²½ìš°ì˜ state transition probablity $p(s'|s=L3,a=Left)$ê°€ ì˜¤ë¥¸ìª½ í‘œì™€ ê°™ë‹¤ê³  í•˜ì.
* *\* ê·¸ë¦¼ì˜ í‘œëŠ” ì •í™•íˆ ë§í•˜ë©´ ìƒíƒœ ì „ì´ì˜ 'probablity distribution'ì´ë‹¤. í™•ë¥  ë³€ìˆ˜ $s$ê°€ ì·¨í•  ìˆ˜ ìˆëŠ” 'ëª¨ë“ 'ê°’ ê°ê°ì˜ í™•ë¥ ì„ ë‚˜íƒ€ëƒˆê¸° ë•Œë¬¸ì´ë‹¤.

$p(s'|s,a)$ê°€ ë‹¤ìŒ ìƒíƒœ $s'$ì„ ê²°ì •í•˜ëŠ” ë°ëŠ” 'í˜„ì¬' state $s$ì™€ action $a$ë§Œì´ ì˜í–¥ì„ ì¤€ë‹¤. ë‹¤ì‹œ ë§í•´ state sransitionì—ì„œëŠ” ê³¼ê±°ì˜ ì •ë³´, ì¦‰ ì§€ê¸ˆê¹Œì§€ ì–´ë–¤ stateë“¤ì„ ê±°ì³ ì™”ê³  ì–´ë–¤ actionì„ ì·¨í•´ ì™”ëŠ”ì§€ëŠ” ì‹ ê²½ ì“°ì§€ ì•ŠëŠ”ë‹¤. ì´ì²˜ëŸ¼ í˜„ì¬ì˜ ì •ë³´ë§Œ ê³ ë ¤í•˜ëŠ” ì„±ì§ˆì„ marcov propertyë¼ê³  í•œë‹¤. MDPëŠ” marcov propertyë¥¼ ë§Œì¡±í•œë‹¤ê³  ê°€ì •í•˜ê³  state transition(and reward)ë¥¼ ëª¨ë¸ë§í•œë‹¤. marcov propertyë¥¼ ë„ì…í•˜ëŠ” ê°€ì¥ í° ì´ìœ ëŠ” ë¬¸ì œë¥¼ ë” ì‰½ê²Œ í’€ê¸° ìœ„í•´ì„œì´ë‹¤. ë§Œì•½ marcov propertyë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ê³¼ê±°ì˜ ëª¨ë“  ìƒíƒœì™€ í–‰ë™ê¹Œì§€ ê³ ë ¤í•´ì•¼ í•´ì„œ, ê·¸ ì¡°í•©ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ë§ì•„ì§„ë‹¤.

## 2.2.2 ë³´ìƒ í•¨ìˆ˜(reward function)
ë‹¤ìŒìœ¼ë¡œ rewardì— ëŒ€í•´ ìƒê°í•´ë³´ì. ì„¤ëª…ì´ ë³µì¡í•´ì§€ì§€ ì•Šë„ë¡ rewardëŠ” deterministicí•˜ê²Œ ì£¼ì–´ì§„ë‹¤ê³  ê°€ì •í•˜ì. agentê°€ state $s$ì—ì„œ action $a$ë¥¼ ìˆ˜í–‰í•˜ì—¬ ë‹¤ìŒ state $s'$ê°€ ë˜ì—ˆì„ ë•Œì˜ rewardë¥¼ $r(s, a, s')$ë¼ëŠ” í•¨ìˆ˜ë¡œ ì •ì˜í•œë‹¤. ê·¸ëŸ¼ reward functionì˜ ì˜ˆë¡œ ì•„ë˜ ê·¸ë¦¼ì„ ë³´ì.
```sheet
{
  classes: {
    center: { textAlign: "center", verticalAlign: "middle", padding: "14px" },
    sp: { border: "none", backgroundColor: "transparent", minWidth: "80px" },
    noBorder: { border: "none", backgroundColor: "transparent" },
    invis: { color: "transparent" },
    bigArrow: { fontSize: "20px", fontWeight: "bold"},
    smallProb: { fontSize: "12px", fontWeight: "bold"},
    vCloser: { verticalAlign: "bottom", paddingTop: "2px", paddingBottom: "0px" },
    redArrow: { color: "red", fontWeight: "bold", fontSize: "22px", textAlign: "right", paddingRight: "0px" },
    bold: { fontWeight: "bold" },
    c1: { backgroundColor: "#aaa" },
    c2: { backgroundColor: "#444" },
    l: {textAlign: "left"},
    r: {textAlign: "right"},
    d: {verticalAlign: "bottom"},
    u: {verticalAlign: "top", paddingTop: "0px"}
  }
}
---
|-| L1 ~.center .noBorder | L2 ~.center .noBorder | L3 ~.center .noBorder | L4 ~.center .noBorder | L5 ~.center .noBorder | Â· ~.invis .noBorder .center |
|-| :--: | :--: | :--: | :--: | :--: | :--: |
|- ~.center|ğŸ ~.redArrow .c1 | âŸ¸ ğŸ‘¾ ~.redArrow .c1 | Â· ~.invis .c1 | Â· ~.invis .c1 |  Â· ~.invis .c1 |  $r(s=L2,a=Left,s'=L1)=1$ ~.bigArrow .noBorder |
```

ê·¸ë¦¼ì—ì„œëŠ” agentê°€ state L2ì—ì„œ(s=L2), action Leftë¥¼ ì„ íƒí•˜ì—¬ state L1ë¡œ ì „ì´ëœ ì˜ˆê°€ ê·¸ë ¤ì ¸ ìˆë‹¤. ì´ ê²½ìš°ì˜ rewardëŠ” $r(s,a,s')$ë¥¼ í†µí•´ 1ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.
ì°¸ê³ ë¡œ ì´ ì˜ˆì—ì„œëŠ” ë‹¤ìŒ state $s'$ë§Œ ì•Œë©´ ë³´ìƒì´ ê²°ì •ëœë‹¤. ì´ë²ˆ ë¬¸ì œì—ì„œëŠ” ì´ë™í•œ ìœ„ì¹˜ì— ì‚¬ê³¼ê°€ ìˆëŠëƒì— ì˜í•´ì„œë§Œ rewardê°€ ê²°ì •ë˜ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ reward functionì„ $r(s')$ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.
* *\* rewardê°€ stochasticí•˜ê²Œ ì£¼ì–´ì§ˆ ìˆ˜ë„ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì–´ë–¤ ì¥ì†Œì— ê°€ë©´ 0.8ì˜ probablityë¡œ ì ì—ê²Œ ìŠµê²©ì„ ë‹¹í•´ -10ì˜ rewardë¥¼ ë°›ëŠ” ê²½ìš°ì´ë‹¤. ë¬¼ë¡  rewardfunction $r(s,a,s')$ê°€ 'Expectation of reward'ë¥¼ ë°˜í™˜í•˜ë„ë¡ ì™¼ìª½ê³¼ ê°™ì´ ê° stateì— ë”°ë¼ 'ë°˜ë“œì‹œ'ì •í•´ì§„ actionì„ í•œë‹¤.

## 2.2.3 ì—ì´ì „íŠ¸ì˜ ì •ì±…(policy of agent)
ë‹¤ìŒìœ¼ë¡œ agentì˜ policyì— ëŒ€í•´ ìƒê°í•´ë³´ì. policyëŠ” agentê°€ actionì„ ê²°ì •í•˜ëŠ” ë°©ì‹ì´ë‹¤. policyì—ì„œ ì¤‘ìš”í•œ ì ì€ agentê°€ 'í˜„ì¬ state'ë§Œìœ¼ë¡œ actionì„ ê²°ì •í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. 'í˜„ì¬ state'ë§Œìœ¼ë¡œ ì¶©ë¶„í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ? ë°”ë¡œ environmentì˜ state transitionì´ marcov propertyì— ë”°ë¼ ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì´ë‹¤. environmentì˜ state transitionì—ì„œëŠ” í˜„ì¬ state $s$ì™€ action $a$ë§Œì„ ê³ ë ¤í•˜ì—¬ ë‹¤ìŒ state $s'$ê°€ ì¡€ì •ëœë‹¤. ì´ìƒì´ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ” 'environmentì— ëŒ€í•´ í•„ìš”í•œ ì •ë³´ëŠ” ëª¨ë‘ í˜„ì¬ environmentì— ìˆë‹¤'ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ agentê°€ 'í˜„ì¬ state'ë§Œìœ¼ë¡œ actionì„ ê²°ì •í•  ìˆ˜ ìˆë‹¤.
* *\* MDPì˜ marcov  propertyë¼ëŠ” íŠ¹ì„±ì€ agentì— ëŒ€í•œ ì œì•½ì´ ì•„ë‹ˆë¼ 'environmentì— ëŒ€í•œ ì œì•½'ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. ì¦‰, marcov propertyë¥¼ ë§Œì¡±í•˜ë„ë¡ 'state'ë¥¼ ê´€ë¦¬í•˜ëŠ” ì±…ì„ì´ environment ìª½ì— ìˆë‹¤ëŠ” ëœ»ì´ë‹¤. agent ê´€ì ì—ì„œ ë³´ë©´ ìµœì„ ì˜ ì„ íƒì— í•„ìš”í•œ ì •ë³´ê°€ ëª¨ë‘ í˜„ì¬ stateì— ë‹´ê²¨ ìˆê¸° ë•Œë¬¸ì— í˜„ì¬ë§Œì„ ë°”ë¼ë³´ê³  í–‰ë™í•  ìˆ˜ ìˆë‹¤.

agentëŠ” í˜„ì¬ stateë¥¼ ë³´ê³  actionì„ ê²°ì •í•œë‹¤. ì´ë•Œ actionì„ ê²°ì •í•˜ëŠ” ë°©ì‹ì¸ policyëŠ” 'deterministic' ë˜ëŠ” 'stochastic'ì¸ ê²½ìš°ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. deterministic policyëŠ” ë‹¤ìŒ ê·¸ë¦¼ì˜ ì™¼ìª½ê³¼ ê°™ì´ ê° stateì— ë”°ë¼ 'ë°˜ë“œì‹œ' ì •í•´ì§„ actionì„ í•œë‹¤.
```sheet
{
  classes: {
    center: { textAlign: "center", verticalAlign: "middle", padding: "14px" },
    sp: { border: "none", backgroundColor: "transparent", minWidth: "80px" },
    noBorder: { border: "none", backgroundColor: "transparent" },
    invis: { color: "transparent" },
    bigArrow: { fontSize: "30px", fontWeight: "bold"},
    smallProb: { fontSize: "12px", fontWeight: "bold"},
    vCloser: { verticalAlign: "bottom", paddingTop: "2px", paddingBottom: "0px" },
    redArrow: { color: "red", fontWeight: "bold", fontSize: "22px", textAlign: "right", paddingRight: "0px" },
    bold: { fontWeight: "bold" },
    c1: { backgroundColor: "#aaa" },
    c2: { backgroundColor: "#444" },
    l: {textAlign: "left"},
    r: {textAlign: "right"},
    d: {verticalAlign: "bottom"},
    u: {verticalAlign: "top", paddingTop: "0px"}
  }
}
---
|-| L1 ~.center .noBorder | L2 ~.center .noBorder | L3 ~.center .noBorder | L4 ~.center .noBorder | L5 ~.center .noBorder | Â· ~.invis .noBorder .center | L1 ~.center .noBorder | L2 ~.center .noBorder | L3 ~.center .noBorder | L4 ~.center .noBorder | L5 ~.center .noBorder |
|-| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|- ~.center| Â· ~.invis .c1 | Â· ~.invis .c1 | âŸ¸ ğŸ‘¾ ~.redArrow .center .c1 | Â· ~.invis .c1 |  Â· ~.invis .c1 |  Â· ~.invis .noBorder | Â· ~.invis .c1 | Â· ~.invis .c1 | âŸ¸ ğŸ‘¾ âŸ¹ ~.redArrow .center .c1 | Â· ~.invis .c1 |  Â· ~.invis .c1 |
|- ~.center| Â· ~.invis .noBorder .center | Â· ~.invis .noBorder .center | Left | Â· ~.invis .noBorder .center | Â· ~.invis .noBorder .center | Â· ~.invis .noBorder .center | Â· ~.invis .noBorder .center | Â· ~.invis .noBorder .center | Left(0.4)   Right(0.6) ~.noBorder .center | Â· ~.invis .noBorder .center | Â· ~.invis .noBorder .center |
```

ì™¼ìª½ ê·¸ë¦¼ì—ì„œ agentëŠ” L3ì— ìˆì„ ë•ŒëŠ” ë°˜ë“œì‹œ ì™¼ìª½ìœ¼ë¡œ ì´ë™í•œë‹¤. ì´ëŸ¬í•œ deterministic policyëŠ” í•¨ìˆ˜ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.
$$
\alpha=\mu(s)
$$
$\mu(s)$ëŠ” ë§¤ê°œë³€ìˆ˜ë¡œ stateë¥¼ ê±´ë„¤ì£¼ë©´ action $a$ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤($\mu$ëŠ” 'ë®¤'ë¼ê³  ë°œìŒí•œë‹¤). ì´ ì˜ˆì—ì„œ $\mu(s=L3)$ëŠ” LeftëŠ” ë°˜í™˜í•œë‹¤.

ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì€ stochastic policyì˜ ì˜ˆì´ë‹¤. agentê°€ ì™¼ìª½ìœ¼ë¡œ ì´ë™í•  probablityëŠ” 0.4ì´ê³ , ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™í•  probablityëŠ” 0.6ì´ë‹¤. ì´ë ‡ê²Œ agentì˜ actionì´ stochasticìœ¼ë¡œ ê²°ì •ë˜ëŠ” policyëŠ” ìˆ˜ì‹ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.
$$
\pi(a|s)
$$
$\pi(a|s)$ëŠ” state $s$ì—ì„œ action $a$ë¥¼ ì·¨í•  probablityë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ê·¸ë¦¼ì˜ ì˜ˆë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.
$$
\pi(a=Left|s=L3)=0.4 \\
$$
$$
\pi(a=Right|s=L3)=0.6
$$
* *\* deterministic policyëŠ” stochastic policyë¡œë„ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ deterministic policyì¸ $\alpha=\mu(s)$ëŠ” state $s$ì—ì„œ action $a$ë¥¼ ìˆ˜í–‰í•  probablityê°€ 1.0ì¸ probablity distributionìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

state transition, reward function, policyë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë³´ì•˜ë‹¤. ë‹¤ìŒ ì ˆì—ì„œëŠ” ì´ ì„¸ ìˆ˜ì‹ì„ ì´ìš©í•´ MDPì˜ ëª©í‘œë¥¼ ì •ì˜í•´ë³´ê² ë‹¤.

# 2.3 MDPì˜ ëª©í‘œ
ì§€ê¸ˆê¹Œì§€ëŠ” environmentì™€ agentì˜ actionì„ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í–ˆë‹¤. ê°„ë‹¨íˆ ë³µìŠµí•˜ë©´, agentëŠ” policy $\pi(a|s)$ì— ë”°ë¼ actionì„ ì·¨í•œë‹¤. ê·¸ actionê³¼ state transition probablity $p(s'|s, a)$ì— ì˜í•´ ë‹¤ìŒ stateê°€ ê²°ì •ëœë‹¤. ê·¸ë¦¬ê³  rewardëŠ” reward function $r(s, a, s')$ê°€ ê²°ì •í•œë‹¤. ì´ í‹€ ì•ˆì—ì„œ optimal policyë¥¼ ì°¾ëŠ” ê²ƒì´ MDPì˜ ëª©í‘œì´ë‹¤. optimal policyë€ returnì´ ìµœëŒ€ê°€ ë˜ëŠ” policyì´ë‹¤(returnì— ëŒ€í•´ì„œëŠ” 2.3.2ì ˆ ì°¸ê³ ).
* *\* agentê°€ deterministic policyë¥¼ ë”°ë¥¸ë‹¤ë©´ $\mu(s)$í•¨ìˆ˜ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ deterministic policyëŠ” stochastic policyë¡œë„ í‘œí˜„ë  ìˆ˜ ìˆìœ¼ë‹ˆ ì—¬ê¸°ì„œëŠ” stochastic policy $\pi(a|s)$ë¥¼ ê°€ì •í•˜ê³  ì§„í–‰í•˜ê² ë‹¤. ê°™ì€ ì´ìœ ë„ environmentì˜ state transisionë„ stochasticì´ë¼ê³  ê°€ì •í•œë‹¤.

ì´ë²ˆ ì ˆì—ì„œëŠ” optimal policyë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ì •í™•í•˜ê²Œ í‘œí˜„í•˜ê³ ì í•œë‹¤. ê·¸ëŸ¬ë ¤ë©´ ë¨¼ì € MDPì˜ ë¬¸ì œê°€ í¬ê²Œ ë‘ ê°€ì§€ë¡œ ë‚˜ë‰œë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œì•„ì•¼ í•œë‹¤. ë°”ë¡œ 'episodic task'ì™€ 'continuous task'ì´ë‹¤.

## 2.3.1 episodic taskì™€ continuous task
MDPëŠ” ë¬¸ì œì— ë”°ë¼ episodic taskì™€ continuous taskë¡œ ë‚˜ë‰œë‹¤. episodic taskëŠ” 'ë'ì´ ìˆëŠ” ë¬¸ì œì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë°”ë‘‘ì€ episodic taskë¡œ ë¶„ë¥˜ëœë‹¤. ë°”ë‘‘ì€ ê²°êµ­ ìŠ¹ë¦¬/íŒ¨ë°°/ë¬´ìŠ¹ë¶€ ì¤‘ í•˜ë‚˜ë¡œ ê·€ê²°ëœë‹¤. ê·¸ë¦¬ê³  ë‹¤ìŒ ëŒ€êµ­ì€ ëŒì´ í•˜ë‚˜ë„ ë†“ì—¬ ìˆì§€ ì•Šì€ ì´ˆê¸° ìƒíƒœì—ì„œ ìƒˆë¡œ ì‹œì‘í•œë‹¤. episodic taskì—ì„œëŠ” ì‹œì‘ë¶€í„° ëê¹Œì§€ì˜ ì¼ë ¨ì˜ ì‹œë„ë¥¼ episodeë¼ê³  í•œë‹¤.

ë‹¤ìŒ ê·¸ë¦¼ê³¼ ê°™ì€ ë¬¸ì œë„ episodic taskì´ë‹¤.
```sheet
{
  classes: {
    center: { textAlign: "center", verticalAlign: "middle", padding: "14px" },
    sp: { border: "none", backgroundColor: "transparent", minWidth: "80px" },
    noBorder: { border: "none", backgroundColor: "transparent" },
    invis: { color: "transparent" },
    bigArrow: { fontSize: "40px", fontWeight: "bold"},
    smallProb: { fontSize: "12px", fontWeight: "bold"},
    vCloser: { verticalAlign: "bottom", paddingTop: "2px", paddingBottom: "0px" },
    redArrow: { color: "red", fontWeight: "bold", fontSize: "22px", textAlign: "right", paddingRight: "0px" },
    bold: { fontWeight: "bold" },
    c1: { backgroundColor: "#aaa" },
    c2: { backgroundColor: "#444" },
    l: {textAlign: "left"},
    r: {textAlign: "right"},
    d: {verticalAlign: "bottom"},
    u: {verticalAlign: "top", paddingTop: "0px"}
  }
}
---
|- ~.noBorder| Start ~.center | Â· ~.invis | Â· ~.invis | Â· ~.invis | Goal ~.center | â¤º ~.bigArrow .center | Start ~.center | Â· ~.invis | Â· ~.invis | Â· ~.invis | Goal ~.center |
|-| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|- ~.center| ğŸ‘¾ ~.redArrow .center .c1 | Â· ~.invis .c1 | Â· ~.invis .c1 | Â· ~.invis .c1 |  Â· ~.invis .c1 | â‡¢ ~.center .bigArrow .noBorder | Â· ~.invis .c1 | Â· ~.invis .c1 | Â· ~.invis .c1 |  Â· ~.invis .c1 | ğŸ‘¾ğŸ ~.redArrow .center .c1 |

```

ì´ ê·¸ë¦¼ì—ì„œëŠ” ì–´ëŠ ê³³ì—”ê°€ ëª©í‘œê°€ ìˆê³  ê·¸ ëª©í‘œì— ë„ë‹¬í•˜ë©´ ëì´ ë‚œë‹¤. ê·¸ í›„ì—ëŠ” ë‹¤ì‹œ ì´ˆê¸° stateì—ì„œ ìƒˆë¡œìš´ episodeê°€ ì‹œì‘ëœë‹¤.

ë°˜ë©´ continuous taskëŠ” 'ë'ì´ ì—†ëŠ” ë¬¸ì œì´ë‹¤. ì˜ˆë¥¼ ë“¤ë©´ ì¬ê³  ê´€ë¦¬ê°€ ìˆë‹¤. ì¬ê³  ê´€ë¦¬ì—ì„œì˜ agentëŠ” ì–¼ë§ˆë‚˜ ë§ì€ ìƒí’ˆì„ êµ¬ë§¤í• ì§€ë¥¼ ê²°ì •í•œë‹¤. íŒë§¤ëŸ‰ê³¼ ì¬ê³ ëŸ‰ì„ ë³´ê³  ìµœì ì˜ êµ¬ë§¤ëŸ‰ì„ ê²°ì •í•´ì•¼ í•œë‹¤(ì¬ê³ ê°€ ë„ˆë¬´ ë§ì•„ì§€ì§€ ì•Šê²Œ ê´€ë¦¬í•˜ë©´ì„œë„ ìƒí’ˆ íŒë§¤ì— ì§€ì¥ì´ ì—†ë„ë¡ í•´ì•¼ í•œë‹¤). ì´ëŸ° ìœ í˜•ì˜ ë¬¸ì œëŠ” ëì„ ì •í•˜ì§€ ì•Šê³  ì˜ì›íˆ ì§€ì†ë  ìˆ˜ ìˆë‹¤.

## 2.3.2 return
ë‹¤ìŒìœ¼ë¡œ ìƒˆë¡œìš´ ìš©ì–´ì¸ returnì„ ì†Œê°œí•˜ê² ë‹¤. ì´ returnì„ ê·¹ëŒ€í™”í•˜ëŠ” ê²ƒì´ agentì˜ ëª©í‘œì´ë‹¤. 
ì‹œê°„ $t$ì—ì„œì˜ stateë¥¼ $S_t$ë¼ê³  í•´ë³´ì($t$ëŠ” ì„ì˜ì˜ ê°’). ê·¸ë¦¬ê³  agentê°€ ì •ì±… $\pi$ì— ë”°ë¼ action $A_t$ë¥¼ í•˜ê³ , reward $R_t$ë¥¼ ì–»ê³ , ìƒˆë¡œìš´ state $S_{t+1}$ë¡œ ì „ì´í•˜ëŠ” íë¦„ì´ ì´ì–´ì§„ë‹¤. ì´ë•Œ return $G_t$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •í•´ì§„ë‹¤.
$$
G_{t}=R_{t}+\gamma R_{t+1}+\gamma^2R_{t+2}+\cdots
$$
ìœ„ì˜ ì‹ê³¼ ê°™ì´ returnì€ agentê°€ ì–»ëŠ” rewardì˜ í•©ì´ë‹¤. í•˜ì§€ë§Œ ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ rewardëŠ” $\gamma$(ê°ë§ˆ)ì— ì˜í•´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¤„ì–´ë“ ë‹¤. ì´ $\gamma$ë¥¼ discount rateë¼ê³  í•˜ë©°, 0.0ì—ì„œ 1.0ì‚¬ì´ ì‹¤ìˆ˜ë¡œ ì •ì˜ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´ discount rateë¥¼ 0.9ë¡œ ì„¤ì •í•˜ë©´ ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ë³€í•œë‹¤.
$$
G_{t}=R_{t}+0.9 R_{t+1}+0.81R_{t+2}+\cdots
$$
discount rateë¥¼ ë„ì…í•˜ëŠ” ê°€ì¥ ì£¼ëœ ì´ìœ ëŠ” continuous taskì—ì„œ returnì´ ë¬´í•œã„·ê°€ ë˜ì§€ ì•Šë„ë¡ ë°©ì§€í•˜ê¸° ìœ„í•´ì„œì´ë‹¤. discount rateê°€ ì—†ë‹¤ë©´, ì¦‰ $\gamma=1$ì´ë¼ë©´ continuous taskì—ì„œ returnì´ ë¬´í•œëŒ€ê¹Œì§€ ëŠ˜ì–´ë‚˜ê²Œ ëœë‹¤. 

ë˜í•œ discount rateëŠ” ê°€ê¹Œìš´ ë¯¸ë˜ì˜ rewardë¥¼ ë” ì¤‘ìš”í•˜ê²Œ ë³´ì´ë„ë¡ í•œë‹¤. ì´ íŠ¹ì„±ì€ ì‚¬ëŒ, ë‚˜ì•„ê°€ ìƒë¬¼ì˜ ë§ì€ í–‰ë™ ì›ë¦¬ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì˜¤ëŠ˜ 1ë§Œì› ë°›ê¸°ì™€ 10ë…„ í›„ì— 2ë§Œì› ë°›ê¸° ì¤‘ ì–´ëŠ ê²ƒì„ ì„ íƒí•  ê²ƒì¸ê°€? discount rateë•Œë¬¸ì— ë¯¸ë˜ ë³´ìƒì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¤„ì–´ë“ ë‹¤ë©´ ì¦‰ì‹œ ì–»ì„ ìˆ˜ ìˆëŠ” rewardì— ë” í° ë§¤ë ¥ì„ ëŠë‚„ ê²ƒì´ë‹¤.

## 2.3.3 state-value function
ë°©ê¸ˆ ì •ì˜í•œ 'return'ì„ ê·¹ëŒ€í™”í•˜ëŠ” ê²ƒì´ agentì˜ ëª©í‘œë¼ê³  í•˜ì˜€ë‹¤. ì—¬ê¸°ì„œ ì£¼ì˜í•  ê²Œ í•˜ë‚˜ ìˆë‹¤. agentì™€ environmentê°€ 'stochastic'í•˜ê²Œ ë™ì‘í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ë‹¤. agentëŠ” ë‹¤ìŒ actionì„ stochasticí•˜ê²Œ ê²°ì •í•  ìˆ˜ ìˆê³ , state ì—­ì‹œ stochasticí•˜ê²Œ ì „ì´ë  ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì–»ëŠ” return ì—­ì‹œ 'stochastic'í•˜ê²Œ ë‹¬ë¼ì§ˆ ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì–´ë–¤ episodeì—¥ì„œëŠ” returnì´ 10.4ì´ê³  ë‹¤ë¥¸ episodeì—ì„œëŠ” returnì´ 8.7ì¸ ì‹ìœ¼ë¡œ ë§ì´ë‹¤. ë¹„ë¡ ê°™ì€ stateì—ì„œ ì‹œì‘í•˜ë”ë¼ë„ stateê°€ episodeë§ˆë‹¤ stochasticí•˜ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.

ì´ëŸ¬í•œ í™•ë¥ ì  ë™ì‘ì— ëŒ€ì‘í•˜ê¸° ìœ„í•´ì„œëŠ” Expectation, ì¦‰ 'Expectation of return'ì„ ì§€í‘œë¡œ ì‚¼ì•„ì•¼ í•œë‹¤. ìƒíƒœ $S_t$ê°€ $s$ì´ê³ ($t$ëŠ” ì„ì˜ì˜ ê°’), agentì˜ policyê°€ $\pi$ì¼ ë•Œ, agentê°€ ì–»ì„ ìˆ˜ ìˆëŠ” ê¸°ëŒ€ ìˆ˜ìµì„ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.
$$
v_{\pi}(s)=\mathbb{E}[G_{t}|S_{t}=s, \pi]
$$
ì´ì²˜ëŸ¼ returnì˜ Expectationì„ $v_{\pi}(s)$ë¡œ í‘œê¸°í•˜ë©° $v_{\pi}(s)$ë¥¼ state-value functionì´ë¼ê³  í•œë‹¤. ìœ„ ì‹ì˜ ìš°ë³€ì—ì„œ agentì˜ policy $\pi$ëŠ” ì¡°ê±´ìœ¼ë¡œ ì£¼ì–´ì§„ë‹¤. ploicy $\pi$ê°€ ë°”ë€Œë©´ agentê°€ ì–»ëŠ” rewardë„ ë°”ë€Œê³  ì´í•©ì¸ returnë„ ë°”ë€Œê¸° ë•Œë¬¸ì´ë‹¤. ì´ëŸ° íŠ¹ì„±ì„ ëª…ì‹œí•˜ê¸° ìœ„í•´ state-value functionì€ $v_{\pi}(s)$ì²˜ëŸ¼ $\pi$ë¥¼ $v$ì˜ ë°‘ìœ¼ë¡œ ì“°ëŠ” ë°©ì‹ì„ ë§ì´ ë”°ë¥¸ë‹¤. ë˜í•œ ìœ„ ì‹ì„ ë‹¤ìŒê³¼ ê°™ì´ ì“°ê¸°ë„ í•œë‹¤.
$$
v_{\pi}(s)=\mathbb{E}_{\pi}[G_{t}|S_{t}=s]
$$
ìœ„ì˜ ì‹ì—ì„œëŠ” ìš°ë³€ì—ì„œ $\pi$ì˜ ìœ„ì¹˜ê°€ $\mathbb{E}_{\pi}$ë¡œ ì˜®ê²¨ê°”ë‹¤. ì˜ë¯¸ëŠ” ì›ë˜ ì‹ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ $\pi$ë¼ëŠ” policyê°€ ì¡°ê±´ìœ¼ë¡œ ì£¼ì–´ì ¸ ìˆìŒì„ ë‚˜íƒ€ë‚¸ë‹¤. ì´ ê¸€ì—ì„œëŠ” ì´ í˜•íƒœë¡œ ì‚¬ìš©í•˜ê² ë‹¤.
* *\* 



```
{
  classes: {
    center: { textAlign: "center", verticalAlign: "middle", padding: "14px" },
    sp: { border: "none", backgroundColor: "transparent", minWidth: "80px" },
    noBorder: { border: "none", backgroundColor: "transparent" },
    invis: { color: "transparent" },
    c: { backgroundColor: "#aaa" },
    big: { fontSize: "30px", fontWeight: "bold"},
    smallProb: { fontSize: "12px", fontWeight: "bold"},
    redArrow: { color: "red", fontWeight: "bold", fontSize: "22px", textAlign: "right", paddingRight: "0px" },
    bold: { fontWeight: "bold" },
    l: {textAlign: "left"},
    r: {textAlign: "right"},
    d: {verticalAlign: "bottom"},
    u: {verticalAlign: "top"}
  }
}
---
| bandit ~ .vMid   | - ~ .c | reward      | < | < |
| ^                | - ~ .c | 1st | 2nd | 3rd |
| :----: ~ .padX_1 .c | - | :----: ~ .padX_2 | :----: ~ .padX_2 | :----: ~ .padX_2 |
| a                | - | 0  | 1 | 5 |
| b                | - | 1  | 0 | 0 |

| Â· ~.invis |
ğŸ
âŸ¸ ğŸ‘¾ ~.redArrow .center .c
```
