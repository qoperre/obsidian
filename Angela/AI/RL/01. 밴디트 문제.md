강화 학습 - 가르치는 사람의 도움 없이도 환경과 상호작용하며 더 나은 해결책을 스스로 학습하는 것

# 1.1 머신러닝 분류, 강화 학습
machine learning - 말 그대로 "Data"를 이용해 Machine을 Learning시키는 것 -> 규칙을 data-based로 찾는 것
- 지도 학습(supervised learning), 비지도 학습(unsupervised learning), 강화 학습(reinforcement learning)으로 나뉨

## 1.1.1 supervised learning
input data-output label을 pair로 제공 (ex, 손글씨 숫자 인식 데이터 : 손글씨 이미지-정답 숫자)
## 1.1.2 unsupervised learning
output label이 존재하지 않음
data속에 들어있는 feature, pattern들을 찾는 데 주로 쓰임
군집화(clustering), 특성 추출, 차원 축소 등
**Labeling(or annotation)** - output label을 사람이 일일이 붙이는 작업 -> 시간 소요 증가

## 1.1.3 reinforcement learning
Agent와 Environment간의 interaction이 존재
Agent - 행동 주체 -> Environment의 State를 Observation하고, 그에 맞는 Action을 취함 -> Reward를 받고, 새로운 State를 관찰
Environment - Agent가 들어있는 환경
**Reinforcement Lerning의 최종 목표 : to maximize its culmulative(누적되는) reward, called the "expected return"**
State VS Observation
State : is a complete description of the state of the world (there is no hidden information). In a fully observed environment.
Observation : is a partial description of the state. In a partially observed environment.
Action Space - the set of all possible actions in an environment -> Discrete/Continuous
# 1.2 Bandit Problem
## 1.2.1 Bandit problem?
정식 명칭 - multi-armed bandit problem
* bandit : 슬롯머신
bandit 각각의 특성이 서로 다름. 이 상황에서 Agent는 정해진 횟수를 플레이함. 어떤 bandit이 승률이 좋은지 알 수 없으므로 직접 플레이해보면서 찾음. Agent의 목표는 코인을 최대한 버는 것
bandit : Environment
게임 플레이 : Acton
코인 : Reward
- Environment의 상태가 변하지 않는다

+좀 더 정확히 아래의 문제들을 해결할 따 사용
1) When multiple conflicting choices must be made under limited resources  
2) When the amount of benefit provided by each choice and the degree of variance are unknown (but uncertainty decreases as the same choice is made multiple times)  
3) When aiming to maximize the expected maximum benefit of these choices

## 1.2.2 What is good bandit?
vandit의 확률분포표(probablity distribution table)이 주어졌을 때, 어떤 vandit을 선택하는 게 좋나? -> Expectation value!


| coins      | 0    | 1    | 5    | 10   |
| ---------- | ---- | ---- | ---- | ---- |
| probablity | 0.70 | 0.15 | 0.12 | 0.03 |
vandit A -> $E(A)=(0 \times 0.70)+(1\times{0}.15)+(5\times 0.12)+(10\times 0.03)=1.05$

| coins      | 0    | 1    | 5    | 10   |
| ---------- | ---- | ---- | ---- | ---- |
| probablity | 0.50 | 0.40 | 0.09 | 0.01 |
vandit B -> $E(B)=(0 \times 0.50)+(1\times{0}.40)+(5\times 0.09)+(10\times 0.01)=0.95$


---

Reward의 expectation value를 value로 부름. 특히, action으로 인한 reward의 expectation value를 action reward로 부름
Action Space - the set of all possible actions in an environment
Discrete/Continuous

## 1.2.3 Mathematical expressions
Reward - $R$
이전 문제에서 $R$은 {0, 1, 5, 10}중 하나, 각 값을 얻을 가능성이 확률로 정해짐 -> random variable

Action - $A$
$A=[a, b]$이면 a, b중 하나 선택
Expectation value - $\mathbb{E}$
$R$의 기댓값 - $\mathbb{E}[R]$, Action $A$를 취했을 때 기댓값 - $\mathbb{E}\left[R|A\right]$ -> $A$가 a였을 경우 - $\mathbb{E}[R|A=a]$ 또는 $\mathbb{E}[R|a]$

행동 가치 - Reward에 대한 Expectation - $Q$ or $q$ -> $Q(A)=\mathbb{E}[R|A]$ 
 - *q일 때에는 "실제 행동 가치", Q일 때에는 '추정치' -> Agent는 q를 알 수 없기 때문에 Q, 추정치 사용

# 1.3 Bandit Algorithm
- 각 bandit의 value를 알면 agent는 가장 좋은 슬롯머신을 구할 수 있다
- 그러나 agent는 각 bandit의 value를 알 수 없다
- 따라서 agent는 각 bandit의 value를 (as accurate as possible)추정해야 한다.

- * supervised learning에서는 문제의 output label이 준비되어 있다. 모델이 예측에 실패해도 정답은 알 수 있다. reinforcement learning의 관점에서, 잘못된 action을 취하더라도 '올바른 행동은 이것이었다'는 것은 알 수 있다는 것이다. 하지만 실제 learning의 agent는 action에 대한 reward만을 얻음. 따라서 agent는 최선의 행동이 무엇인지 reward를 단서삼아 스스로 추론해야 함.

## 1.3.1 value estimation method
vandit a, b를 1회씩 플레이하는 action의 reward가 다음과 같다면
```sheet
{
  classes: {
    padX_1: { paddingLeft: "50px", paddingRight: "50px" },
    padX_2: { paddingLeft: "20px", paddingRight: "20px" },
    vMid: { verticalAlign: "middle", textAlign: "center" }, 
    c: {backgroundColor: "#444"}
  }
}
---
| bandit ~ .vMid   | - ~ .c | reward           |
| ^                | - ~ .c | 1st              |
| :----: ~ .padX_1 .c | - | :----: ~ .padX_2 |
| a                | - | 0                |
| b                | - | 1                |
```

bandit a에서는 reward 1, b에서는 0을 얻는다. 현제 예에서 '획득한 보상의 평균'을 가치 추정치(Q)로 생각해보면 (아직은 처음이기 때문에) a의 value는 0, b의 value는 1이다.
물론 1번의 action 후의 추정은 trustable하지 못함. 그러나 action 횟수가 늘어날 수록 추정치가 정확해짐. 예를 들어
```sheet
{
  classes: {
    padX_1: { paddingLeft: "50px", paddingRight: "50px" },
    padX_2: { paddingLeft: "20px", paddingRight: "20px" },
    vMid: { verticalAlign: "middle", textAlign: "center" }, 
    c: {backgroundColor: "#444"}
  }
}
---
| bandit ~ .vMid   | - ~ .c | reward      | < | < |
| ^                | - ~ .c | 1st | 2nd | 3rd |
| :----: ~ .padX_1 .c | - | :----: ~ .padX_2 | :----: ~ .padX_2 | :----: ~ .padX_2 |
| a                | - | 0  | 1 | 5 |
| b                | - | 1  | 0 | 0 |
```
이와 같은 결과를 얻었을 시, $Q(a)$는
$$
Q(a)=\frac{0+1+5}{3}=2
$$
이기 때문에 action당 평균적으로 +2의 reward를 받았다고 불 수 있고, $Q(b)$는
$$
Q(b)=\frac{1+0+0}{3}=0.33\cdots
$$
이어서, bandit a다 더 좋을 것이라고 예측할 수 있다.
- * bandit에 실제 action을 취하여 얻은 reward는 어떤 probability distribution에서 생성된 sample임. 따라서 reward들의 평균은 sample mean(표본 평균)이라고 알 수 있고, 이는 law of great numbers(큰 수의 법칙)에 따라 sample의 수를 늘리면 늘릴수록 q에 가까워지고, $\infty$가 되면 q와 같아짐.

## 1.3.2 평균을 구하는 코드
bandit 1대에서만 n번의 action을 취할 때의 action reward를 추정해보자. 실제 보상이 $R_1, R_2, \cdots, R_n$이라 한다면 n번 행동했을 때의 action-value Estimate $Q_n$은 다음의 수식으로 나타나진다.
$$
Q_{n}=\frac{R_{1}+R_{2}+\cdots+R_{n}}{n}
$$
이를 python 코드로 구현하면(n=10으로 가정)
```python
import numpy as np

np.random.seed(42) # 시드 고정
rewards = []
n = 10

for i in range(1, n + 1):
	reward = np.random.rand() # 보상(무작위 수로 시뮬레이션)
	rewards.append(reward)
	Q = sum(rewards) / i
	print(Q)
```
---
출력 결과
```bash
0.3745401188473625
0.6626272126286393
0.6857494556895612
0.6639767128164301
0.5623850983416314
0.4946533353407266
0.4322862320303656
0.4865224712484368
0.4992549757478559
0.5201367359526748
```
0.0 이상 1.0 미만의 무작위 수를 만들어 보상으로 활용함. 그 후 얻은 보상을 rewards에 추가하고, $Q_n$의 식에 대입해 계산

최적화? 
n이 증가할수록 rewards의 원소 수 증가, sum의 cost도 증가함. 따라서 최적화 필요
이를 위해 이전의 가치 추정치인 $Q_{n-1}$를 구한 후, 관계식으로 $Q_n$ 계산
$$
Q_{n-1}=\frac{R_{1}+R_{2}+\cdots+R_{n-1}}{n-1}
$$
양변에 $n-1$을 곱하면
$$
R_{1}+R_{2}+\cdots+R_{n}=(n-1)Q_{n-1}
$$
따라서 $Q_n$은
$$
Q_{n}=\frac{R_{1}+R_{2}+\cdots+R_{n}}{n} 
=\frac{1}{n}\left(\underbrace{R_{1}+R_{2}+\cdots+R_{n-1}}_{(n-1)Q_{n-1}}\,+R_{n}\right)
=\frac{1}{n}\{(n-1)Q_{n-1}+R_{n}\}
=\left(1-\frac{1}{n}\right)Q_{n-1}+\frac{1}{n}R_{n}
$$
이렇게 관계식 형태로 도출하면 $Q_n$을 구하기 위해 지금까지의 모든 rewards $(R_1, R_2, \cdots, R_n)$을 매번 사용하지 않아도 됨
조금 더 변형하면
$$
Q_{n}=\left( 1-\frac{1}{n} \right)Q_{n-1}+\frac{1}{n}R_{n}
=Q_{n-1}+\frac{1}{n}(R_{n}-Q_{n-1})
$$
이 식으로 보아, $Q_n$은 $Q_{n-1}$에 어떠한 값을 더함으로써 구할 수 있다. 정확히는 $(R_n-Q_{n-1}$에 $\frac{1}{n}$을 곱한 만큼 이동한다. 여기서 $R_n$방향으로 얼마나 움직일지는 $\frac{1}{n}$이 결정하기 때문에 이를 learning rate라 부름

이를 바탕으로 코드를 수정하면
```python
Q = 0
n = 10

for i in range(1, n + 1):
	reward = np.random.rand()
	Q = Q + (reward - Q) / i # Q_new = Q + (reward - Q) / i, Q = Q_new
	print(Q)
```
이렇게 순차적으로 증가시키면서 무언가를 구할 수 있는 방식을 incremental implementation(증분 구현)이라고 함

## 1.3.3 Policy of Agent
이제 value를 estimate할 수 있게 되었으니, agent가 취해야 할 전략(policy)를 결정할 차례
그냥 action의 reward가 가장 좋았던 bandit을 계속 선택하면 안되는건가? -> greedy policy
--> 위에서의 bandit a, b 예시의 경우, greedy policy를 이용하면 이후로는 계속 b만 선택함 -> 더 나쁜 결과
why? bandit의 value estimate에 "uncertainty(불확정성)"이 존재하기 때문!

이를 고려하면 agent가 선택할 수 있는 action의 경우의 수는
 - exploitation(활용) : 현재까지의 action을 바탕으로 가장 reward가 높았던 bandit에 action을 취함(greedy policy)
 - exploration(탐색) : bandit의 가치를 정확하게 추정하기 위해 다양한 bandit을 시도
이 둘은 conflict하기에 둘 중 하나는 희생해야 함
---> reinforcement의 algoritm은 결국 **exploitation과 exploration의 균형을 어떻게 잡느냐**

가장 기본적 알고리즘 - $\varepsilon$-greedy policy
$\varepsilon$-greedy policy - $\varepsilon$의 probablity로 exploration, $1-\varepsilon$의 probablity로 exploitation의 action을 취하는 알고리즘

# 1.4 Implementation of Bandit Algorithm
현 상황에서는 bandit의 최대 reward를 1로 제한(0 or 1), bandit의 winning probablity(value)는 고정되어 있다고 가정
bandit의 수는 총 10대, agent는 각각의 bandit의 value를 알지 못함. 따라서 경험을 토대로 value가 높은 bandit을 찾아야 함

## 1.4.1 Implementation of Bandit
value는 무작위로 설정
```python
import numpy as np

class Bandit:
	def __init__(self, arms=10): # arms = bandit 대수
		self.rates = np.random.rand(arms) # bandit 각각의 value 설정
		
	def play(self, arm):
		rate = self.rates[arm]
		if rate > np.random.rand():
			return 1
		else:
			return 0
```
 \_\_init__() method에서 매개변수로 arm(bandit의 대수) 설정
 play(self, arm) method에서 arm은 몇 번째 vandit에 action을 취할 것인지, rate라는 난수를 받아 value(승률)가 난수보다 크면 +1 reward, 아니면 +0 reward 제공

bandit을 써보면
```python
bandit = Bandit()

for _ in range(3):
	print(bandit.play(0))
```
---
출력 결과
```bash
1
0
1
```
 0번째 bandit에 3회 연속 action을 취하여 얻은 reward를 출력함

## 1.4.2 Implementation of Agent
복습 겸 0번째 bandit의 value를 추정해 보면
```python
bandit = Bandit()
Q = 0
n = 10 # 10번 반복

for i in range(1, n + 1):
	reward = bandit.play(0) # 0번째 bandit에 action을 취함(bandit 플레이)
	Q += (reward - Q) / n # value 추정치 갱신
	print(Q)
```
10대 bandit의 각각의 value Expectation을 구하는 코드를 작성해 보면
```python
bandit = Bandit()
Qs = np.zeros(10) # 각 banit의 value expectation
ns = np.zeros(10) # 각 bandit의 action 횟수

for i in range(10):
	action = np.random.randint(10) # 무작위 action(임의의 bandit 선택)
	reward = bandit.play(action)
	
	ns[action] += 1 # action번째 bandit 플레이 횟수 증가
	Qs[action] += (reward - Qs[action]) / ns[action]
	print(Qs)
```

지금까지의 지식을 바탕으로 Agent class를 구현해 보면
```python
class Agent:
	def __init__(self, epsilon, action_size=10):
		self.epsilon = epsilon # 무작위로 exploration할 probablity
		self.Qs = np.zeros(action_size)
		self.ns = np.zeros(action_size)
		
	def update(self, action, reward): # bandit의 value 추정
		self.ns[action] += 1
		self.Qs[action] += (reward - self.Qs[action]) / self.ns[action]
		
	def get_action(self): # action 선택(epsilon-greedy policy)
		if np.random.rand() < self.epsilon:
			return np.random.randint(0, len(self.Qs)) # exploration
		return np.argmax(self.Qs) # greedy
```
초기화 매개변수 epsilon - $\varepsilon$-greedy 정잭에 따라 exploration할 probablity
ex) epsilon = 0.1 -> 10% probablity로 exploration
action size - agent의 action space의 크기 -> bandit의 대수
value 추정 - update() method
get_action - $\varepsilon$-greedy policy로 action을 선택하는 method
self.epsilon의 probablity로 exploration을 선택하고, 그 위에는 exploitation
* * np.argmax(self.Qs) - self.Qs에서 값이 가장 큰 원소의 index 반환

## 1.4.3 Excute
Bandit class, Agent class를 이용해 action을 1000번 수행했을 때의 reward를 보는 코드
```python
import matplotlib.pyplot as plt

steps = 1000
epsilon = 0.1

bandit = Bandit()
agent = Agent(epsilon)
total_reward = 0
total_rewards = [] # reward 합
rates = []         # value(승률)

for step in range(steps):
	action = agent.get_action()  # 1. action 선택
	reward = bandit.play(action) # 2. 실제로 플레이하고 보상을 받음
	agent.update(action, reward) # 3. 행동과 보상을 통해 학습
	total_reward += reward
	
	total_rewards.append(total_reward)      # 현재까지의 reward 합 저장
	rates.append(total_reward / (step + 1)) # 현재까지의 value(승률) 저장
	
print(total_reward)

#그래프 그리기: 단계별 reward 총합
plt.ylabel('Total reward')
plt.xlabel('Steps')
plt.plot(total_rewards)
plt.show()

#그래프 그리기: 단계별 value 추정치
plt.ylabel('Rates')
plt.xlabel('Steps')
plt.plot(rates)
plt.show()
```
---
출력 결과
```bash
882
```
![[Pasted image 20250927202311.png]]
![[Pasted image 20250927202303.png]]

---
for문 안에서 agent와 environment가 interact한다. agent가 action을 선택한 후, 실제 reward를 얻는다. 그리고 agent가 action과 reward 사이의 관계를 학습한다. 이 과정을 1000번 반복하면서 각 단계까지 얻은 보상의 합을 total_rewards 리스트에 추가하고 그동안의 승률은 rate 리스트에 저장된다.

코드 실행 결과 최종 reward의 합은 882. 1000번 중 882번이 +1 reward를 받음. 첫 번째 그래프를 보면, reward의 총합은 꾸준히 증가하나 증가 방식의 특징은 알기 어려움

두 번째 그래프를 보면, 초반에 승률이 훅 떨어지고 이후 완만하게 증가함. 학습은 제대로 이루어진 것으로 보임.

## 1.4.4 알고리즘의 평균적인 특성

![[Pasted image 20250927213501.png]]
10번을 돌린 결과임. 코드에 무작위성이 있기 때문에 매번 결과가 다름

알고리즘의 성능을 평가할 때에는 "평균적인 우수성"으로 판단
```python
runs = 200
steps = 1000
epsilon = 0.1
all_rates = np.zeros((runs, steps)) # (200, 1000) 배열
  
for run in range(runs):
  bandit = Bandit()
  agent = Agent(epsilon)
  total_reward = 0
  rates = [] 

  for step in range(steps):
    action = agent.get_action() 
    reward = bandit.play(action) 
    agent.update(action, reward)
    total_reward += reward
    rates.append(total_reward / (step + 1)) 

  all_rates[run] = rates # reward 결과 기록

avg_rates = np.average(all_rates, axis=0)

#그래프 그리기: 단계별 승률(200번 실험 후 평균)
plt.ylabel('Rates')
plt.xlabel('Steps')
plt.plot(avg_rates)
plt.show()
```
---
실행 결과
![[Pasted image 20250927214408.png]]
$\varepsilon$값을 바꾸면 결과도 달라진다.
![[Pasted image 20250927214900.png]]
$\varepsilon=0.3$이면 처음에는 급격히 증가하다 급격히 꺾임. 30%라는 높은 probablity로 exploration을 시도하기에 최적의 bandit을 선택하는 exploitation의 비율이 너무 낮기 때문으로 보여짐.
반면 $\varepsilon=0.01$인 경우는 상승률이 매우 느림. 최적의 bandit을 찾을 확률이 너무 낮았기 때문으로 보여짐.
$\varepsilon=0.1$일 때가 'exploration과 exploitation의 균형'을 가장 잘 맞춘 것으로 보여짐

# 1.5 Non-stationary problem(비정상 문제)
지금까지의 bandit problem은 reward의 probablity distributiob이 변하지 않는 stationary problem에 속함. 이는
```python
class Bandit:
  def __init__(self, arms=10): # arms = bandit 대수
    self.rates = np.random.rand(arms) # bandit 각각의 value 설정
  def play(self, arm):
    rate = self.rates[arm]
    if rate > np.random.rand():
      return 1
    else:
      return 0
```
에서 확인 가능. self.rates는 초기화 후 변하지 않음 -> stationary problem!

non-stationary problem으로 변환하려면
```python
class NonStatBandit:
	def __init__(self, arms=10):
		self.arms = arms
		self.rates = np.random.rand(arms)
	
	def play(self, arm):
		rate = self.rates[arm]
		self.rates += 0.1 * np.random.randn(self.arms) # noise 추가
		if rate > np.random.rand():
			return 1
		else:
			return 0 
```
* * np.random.randn()은 평균 0, 표준편차 1의 normal distribution에서 무작위 수를 생성함.
## 1.5.1 To solve non-stationary problem
복습부터 하면, bandit의 value를 추정하기 위해 sample mean을 계산했다. 실제 얻은 reward를 $R_1, R_2, \cdots, R_n$이라 하면 sample mean은 다음의 수식으로 표현된다.
$$
Q_{n}=\frac{R_{1}+R_{2}+\cdots+R_{n}}{n}=\frac{1}{n}R_{1}+\frac{1}{n}R_{2}+\cdots+\frac{1}{n}R_{n}
$$
여기서 봐야할 점은 모든 보상 앞에 $\frac{1}{n}$이 붙어 있다는 사실이다. 이 $\frac{1}{n}$을 각 보상에 대한 '가중치'로 볼 수 있다.

현재는 모든 보상에 똑같은 가중치가 부여된다. 새로 얻은 보상이든 오래 전에 얻은 보상이든 모두 동등하게 취급한다는 것이다. 그런데 이것은 non-stationary problem에서는 적합하지 않다. non-stationary problem에서는 시간이 흐르면 environment가 변하기 때문에 과거 reward에 대한 중요도는 점점 낮아져야 한다. 또한, 반대로 최근 얻은 reward의 가중치는 점점 커져야 한다.

sample mean을 incremental implementation으로 나타내면
$$
Q_{n}=Q_{n-1}+\frac{1}{n}(R_{n}-Q_{n-1})
$$
이었다. 이 $\frac{1}{n}$을 고정값 $\alpha$로 바꾼다($0<\alpha<1$). 수식으로는 다음과 같다.
$$
Q_{n}=Q_{n-1}+\alpha(R_{n}-Q_{n-1})
$$
이를 적용하면 오래전에 받은 reward일수록 가중치가 작아진다(Exponential하게). 수식으로 설명하면
$$
Q_{n}=Q_{n-1}+\alpha(R_{n}-Q_{n-1})=\alpha R_{n}+Q_{n-1}-\alpha Q_{n-1}=\alpha R_{n}+(1-\alpha)Q_{n-1}
$$
또 n에 n-1을 대입하면
$$
Q_{n-1}=Q_{n-2}+\alpha(R_{n-1}-Q_{n-2})
$$
이를 $Q_{n}=\alpha R_{n}+(1-\alpha)Q_{n-1}$에 대입해주면

$$
Q_{n}=\alpha R_{n}+(1-\alpha)Q_{n-1}
$$
$$
=\alpha R_{n}+(1-\alpha)(Q_{n-2}+\alpha(R_{n-1}-Q_{n-2}))
$$
$$
= \alpha R_{n}+\alpha(1-\alpha)R_{n-1}+(1-\alpha)^2Q_{n-2}
$$
이를 $Q_{n-2}$에 반복해 주면
$$
Q_{n}= \alpha R_{n}+\alpha(1-\alpha)R_{n-1}+\alpha(1-\alpha)^2Q_{n-2}+(1-\alpha)^3Q_{n-3}
$$
이것을 n번 반복하면
$$
Q_{n}= \alpha R_{n}+\alpha(1-\alpha)R_{n-1}+\cdots+\alpha(1-\alpha)^{n-1}Q_{1}+(1-\alpha)^nQ_{0}
$$
이를 살펴보면 가중치가 exponential하게 감소함을 알 수 있다.
 - $R_n$의 가중치=$\alpha$
 - $R_{n-1}$의 가중치=$\alpha(1-\alpha)$
 - $R_{n-2}$의 가중치=$\alpha(1-\alpha)^2$
 - $R_{n-3}$의 가중치=$\alpha(1-\alpha)^3$
 - ......

이런 식으로 exponential하게 감소하기 따문에 이 계산을 exponential moving average(지수 이동 평균) 또는 exponential weighted moving average(지수 가중 이동 평균)이라고 한다.
* \* *주의점: 우리의 식에서 $Q_n$을 구하는 데에 $Q_0$가 사용되었다. 이는 action value의 초깃값으로, 우리가 설정하는 것이다. 즉, $Q_n$의 값은 우리가 설정한 초깃값에 영향을 받는다. 다시 말해, 우리가 설정한 값에 따라 학습 결과에 bias가 생긴다. 반면 saple mean의 경우 첫 번째 reward를 받을 시 초깃값은 '사라진다'라고 생각해 $Q_0$에 영향을 받지 않는다.

## 1.5.2 non-stational problem 풀기
AlphaAgent class를 새로 만들면 다음과 같다.
```python
class AlphaAgent:
	def __init__(self, epsilon, alpha, action_size=10):
		self.epsilon = epsilon 
		self.Qs = np.zeros(action_size)
		self.alpha = alpha # 고정값으로 설정
		
	def update(self, action, reward):
		# alpha로 갱신
		self.Qs[action] += (reward - self.Qs[action]) * alpha
		
	def get_action(self): 
		if np.random.rand() < self.epsilon:
			return np.random.randint(0, len(self.Qs)) 
		return np.argmax(self.Qs) 
```
Expectation을 계산할 때 $\frac{1}{n}$을 $\alpha$로만 바꿔 준 것이다.
결과는 다음과 같다.
![[Pasted image 20250927225333.png]]
exponential weighted moving update가 더 적합하다는 사실을 알 수 있다.

# 1.6 정리
이번 장에서는 reinforcement learning의 기초를 알아보았다. reinforcement learning은 machine learning의 한 분야이지만 agent와 envirionment간의 상호작용이 일어난다는 점에서 supervised learning이나 non-supervised learning과는 차이가 있다. agent는 자신의 action에 대해 reward를 받고 culmulative reward를 maximize하는 pattern을 익히는 것을 목표로 삼는다.

이어서 bandit problem에 대해 알아보았다. bandit problem을 풀기 위한 algoritm은 '여러 선택지들 중 최선의 선택지를 고르는 문제'에 똑같이 적용할 수 있다. 예를 들어 '매출에 기여하는 웹 디자인을 선택하는 문제'나 '효능이 가장 좋은 약을 선택하는 문제'에서도 활용될 수 있다.

bandit problem, 그리고 reinforcement lerning에서는 'exploration과 exploitation의 균형을 맞추는 일'이 중요하다. 이번에는 이를 구현하기 위한 알고리즘으로 $\varepsilon$-greedy policy를 배웠다. $\varepsilon$-greedy policy는 지금까지의 경험을 exploitation하는 동시에 (가끔은) greedy하지 않은 방법으로 더 나은 선택지가 있는지 exploration한다. 이를 통해 bandit problem을 효율적으로 해결해 볼 수 있었다. 참고로 bandit algorithm으로는 이 외에도 UCP(Upper Confidence Bound) algorithm, Gradient Bandit algoritm등이 있다. 

또한 Average에 관해서도 배웠다. 두 가지 평균이 등장했는데, 'sample mean(표본평균)'과 'exponential weigted moving average(지수 가중 이동 평균)'이다. action value의 Estimated value(추정값)은 위 두 가지 average들을 사용하여 구할 수 있다. 어떤 방ㅇ식을 사용할지는 problem의 성격이 결정한다. stational problem에서는 sample mean을, non-stational problem에서는 exponential weighted moving average를 사용한다. 또한 두 average는 다음과 같이 incremental implement로 계산된다.
 - sample mean: $Q_{n}=Q_{n-1}+\frac{1}{n}(R_{n}-Q_{n-1})$
 - exponential weighted moving average: $Q_{n}=Q_{n-1}+\alpha(R_{n}-Q_{n-1})$


# Bonus
# A.1 다른 Exploration Strategy들
앞서 본 $\varepsilon$-greedy policy는 간단하고 효과적이지만, 탐색 비율이 고정되어 있기 때문에 한계가 존재한다. 이번에는 이를 보완할 수 있는 다른 알고리즘들을 살펴보자.

## A.1.1 UCB (Upper Confidence Bound)
$\varepsilon$-greedy는 단순히 무작위로 탐색을 시도했지만, UCB는 **불확실성이 큰 action을 더 자주 시도**하도록 설계된 알고리즘이다.
- 아이디어
    - 각 bandit의 추정치 $Q(a)$는 평균 보상.
    - 하지만 시도 횟수가 적은 action은 아직 불확실성이 크다.
    - 따라서 "평균 보상 + 불확실성 보정항"을 기준으로 선택한다.
- 수식   $$  
    A_t = \arg\max_a \left[ Q(a) + c \sqrt{\frac{\ln t}{N(a)}} \right]  
    $$
    - $Q(a)$ : 현재까지의 보상 추정치
    - $N(a)$ : action $a$가 선택된 횟수
    - $t$ : 전체 스텝
    - $c$ : 탐색 정도를 조절하는 상수

즉, 평균 보상($Q(a)$)이 크거나, 혹은 선택 횟수가 적어서 불확실성이 큰 action을 선택하는 경향이 있다.
- 특징
    - $\varepsilon$-greedy보다 "지능적인 탐색"을 수행한다. 
    - 너무 적게 선택된 action은 반드시 더 탐색된다 → "Exploration 보장"
```python
import numpy as np

class UCBAgent:
    def __init__(self, c, action_size=10):
        self.c = c
        self.Qs = np.zeros(action_size)
        self.ns = np.zeros(action_size) + 1e-5  # 0으로 나누는 것 방지
        self.t = 0

    def update(self, action, reward):
        self.ns[action] += 1
        self.Qs[action] += (reward - self.Qs[action]) / self.ns[action]

    def get_action(self):
        self.t += 1
        ucb_values = self.Qs + self.c * np.sqrt(np.log(self.t) / self.ns)
        return np.argmax(ucb_values)
```
---

## A.1.2 Thompson Sampling
Thompson Sampling은 **Bayesian 확률 모델**을 기반으로 한 탐색 방법이다.
- 아이디어
    - 각 action의 보상 확률을 **확률 분포(예: Beta 분포)** 로 추정한다.
    - 각 step에서 이 분포에서 값을 샘플링하고, 가장 큰 샘플을 뽑은 action을 선택한다.

- 예시 (이진 보상 0/1 bandit인 경우)
    - 각 bandit $a$에 대해 **Beta 분포**를 사용한다: $Beta(\alpha_a, \beta_a)$
    - 보상을 얻으면 $\alpha_a \leftarrow \alpha_a + 1$
    - 보상을 못 얻으면 $\beta_a \leftarrow \beta_a + 1$
    - action 선택 시에는 각 분포에서 샘플을 뽑아, 가장 큰 값을 준 action을 선택한다. 
- 특징
    - 자연스럽게 exploration과 exploitation의 균형이 잡힌다.
    - 실제 서비스(광고 클릭 최적화, 추천 시스템 등)에서 매우 많이 쓰인다.
    - $\varepsilon$-greedy보다 더 빠르게 최적 action에 수렴하는 경향이 있음.

```python
class ThompsonAgent:
    def __init__(self, action_size=10):
        self.alphas = np.ones(action_size)
        self.betas = np.ones(action_size)

    def update(self, action, reward):
        if reward == 1:
            self.alphas[action] += 1
        else:
            self.betas[action] += 1

    def get_action(self):
        samples = np.random.beta(self.alphas, self.betas)
        return np.argmax(samples)
```
---
## A.1.3 Gradient Bandit 
 - 아이디어
	 - 액션의 “가치(값) Q”를 직접 추정하지 않고, 각 액션의preference H(a)를 학습한다.  
	 - 선호도를 softmax distribution으로 바꿔 정책을 만들고, 기대보상을 올리는 방향으로 $H$를 경사상승(gradient ascent)한다.

- 정책(확률)  
  $$
    \pi(a)=\frac{e^{H(a)}}{\sum_{a'} e^{H(a')}}  
    $$ 
- 업데이트(이진/일반 보상 공통, REINFORCE의 bandit 형태) 
$$
H_{t+1}(a) =
\begin{cases}
H_t(a) + \alpha (R_t - b_t)\big(1 - \pi_t(a)\big), & \text{if } a = a_t \\
H_t(a) - \alpha (R_t - b_t)\pi_t(a), & \text{if } a \neq a_t
\end{cases}
$$
    - $\alpha$: 학습률
    - $R_t$: Step t에서 받은 reward
    - $b_t$: **baseline(기준)**. 보통 **평균 보상**을 사용해 분산을 낮춘다(없어도 됨).
    - $\pi_t(a)$: t스텝의 softmax 정책 확률

**해석**
- 선택한 액션이 기대보다 보상이 크면$(R_t>b_t)$ 그 액션의 선호도를 올리고, 다른 액션은 조금 내린다.
- 반대로 기대보다 작으면 선호도를 낮춘다.
- softmax로 확률을 만들기 때문에, **탐색이 자동으로 내장**되어 있고 보상이 높았던 액션에 점점 확률이 몰린다.
```python
class GradientAgent:
    def __init__(self, alpha=0.1, action_size=10, use_baseline=True):
        self.alpha = alpha
        self.H = np.zeros(action_size)      # preferences
        self.use_baseline = use_baseline
        self.avg_reward = 0.0               # baseline (running average)
        self.t = 0

    def _policy(self):
        z = self.H - np.max(self.H)         # 안정화
        exp = np.exp(z)
        return exp / np.sum(exp)

    def get_action(self):
        probs = self._policy()
        return np.random.choice(len(self.H), p=probs)

    def update(self, action, reward):
        self.t += 1
        baseline = self.avg_reward if self.use_baseline else 0.0
        if self.use_baseline:
            self.avg_reward += (reward - self.avg_reward) / self.t

        probs = self._policy()
        for a in range(len(self.H)):
            if a == action:
                self.H[a] += self.alpha * (reward - baseline) * (1 - probs[a])
            else:
                self.H[a] -= self.alpha * (reward - baseline) * probs[a]
```
---
## A.1.4 Exploration 전략 비교(statistical)
- **ε-greedy**: 단순, 구현이 쉽지만 exploration 비율이 고정됨.
- **UCB**: 선택 횟수가 적은 action을 자동으로 더 자주 탐색 -> 이론적으로 성능이 보장됨.
- **Thompson Sampling**: 확률적 탐색, 실제 응용에서 가장 좋은 성능을 보이는 경우가 많음.
![[Pasted image 20250927235856.png]]
 UCB가 가장 천천히 수렴하는 것을 볼 수 있는데, 이것은 선택 횟수가 더 적은 action을 선택하는 UCP의 특성 때문이다. 또한 Gradient Bandit algoritm 또한 빠르지 않은데, 이는 각 Action을 softmax policy로 선택하고, 지금은 보상이 0/1로 약한 보상만 들어오기 때문이라고 추측된다.

## A.1.5 Exploration 전략 비교(non-stational)
non-stational problem에서의 중요한 포인트는 **과거의 보상 기록을 얼마나 신뢰할 것인가**이다. stationary problem에서는 sample mean을 쓰면 오래된 보상도 계속 평균에 포함시켜도 괜찮다. 하지만 non-stationary problem에서는 오래된 보상은 거의 의미가 없고, 최근 보상에 더 큰 가중치를 두는 쪽이 유리하다. 그래서 $\frac{1}{n}$ 대신 고정된 step-size $\alpha$를 쓰는 **exponential moving average (EMA)** 방식이 적합하다.
![[Pasted image 20250927235923.png]]
- **ε-greedy (EMA 버전)**  
    non-stationary 문제에서는 sample mean 대신 EMA로 Q 값을 업데이트해야 한다. 이렇게 하면 최근 reward에 더 민감하게 반응할 수 있어서 환경 변화에 적응한다. 하지만 여전히 ε 값에 따라 성능이 크게 좌우된다.
- **UCB (EMA 버전)**  
    UCB는 기본적으로 선택 횟수가 적은 action을 계속 보정하기 때문에, stationary 문제에서는 균형 잡힌 탐색이 가능하다. 하지만 non-stationary 환경에서는 “이전에 충분히 탐색한 action”이 다시 나빠졌을 때 이를 빠르게 감지하지 못한다. EMA를 적용하면 어느 정도 적응력이 생기지만, 여전히 변화에 뒤늦게 반응하는 경향이 있다.
- **Thompson Sampling (discounted update)**  
    원래의 Thompson Sampling은 누적 보상으로 posterior를 업데이트하기 때문에 stationary 문제에 더 적합하다. non-stationary 문제에서는 discounting(할인)을 적용해서 오래된 정보의 영향을 줄이는 방식이 필요하다. 이렇게 하면 최근의 reward 분포를 더 잘 반영할 수 있고, ε-greedy보다 안정적으로 변화에 적응한다.
- **Gradient Bandit**  
    Gradient Bandit은 구조적으로 EMA 성질을 가지고 있다. 매 step마다 보상과 baseline을 기준으로 preference를 업데이트하고, softmax 확률로 action을 고르기 때문에 최근 reward에 더 민감하다. 따라서 non-stationary 환경에서도 비교적 잘 적응하는 편이다. 다만 보상이 단순할수록 업데이트 폭이 작아 학습이 느릴 수 있다는 점은 stationary 환경과 마찬가지다.

# A.2 비교분석 코드
## A.2.1 Stational
```python
import numpy as np
import matplotlib.pyplot as plt

# ----------------------------
# Bandit Environment
# ----------------------------
class Bandit:
    def __init__(self, arms=10):
        self.rates = np.random.rand(arms)  # 각 bandit의 승률 (0~1)
    def play(self, arm):
        rate = self.rates[arm]
        return 1 if np.random.rand() < rate else 0

# ----------------------------
# ε-greedy Agent
# ----------------------------
class EpsGreedyAgent:
    def __init__(self, epsilon, action_size=10):
        self.epsilon = epsilon
        self.Qs = np.zeros(action_size)
        self.ns = np.zeros(action_size)

    def update(self, action, reward):
        self.ns[action] += 1
        self.Qs[action] += (reward - self.Qs[action]) / self.ns[action]
    def get_action(self):
        if np.random.rand() < self.epsilon:
            return np.random.randint(len(self.Qs))  # exploration
        return np.argmax(self.Qs)  # exploitation
        
# ----------------------------
# UCB Agent
# ----------------------------
class UCBAgent:
    def __init__(self, c, action_size=10):
        self.c = c
        self.Qs = np.zeros(action_size)
        self.ns = np.zeros(action_size) + 1e-5  # division by zero 방지
        self.t = 0

    def update(self, action, reward):
        self.ns[action] += 1
        self.Qs[action] += (reward - self.Qs[action]) / self.ns[action]
    def get_action(self):
        self.t += 1
        ucb_values = self.Qs + self.c * np.sqrt(np.log(self.t) / self.ns)
        return np.argmax(ucb_values)

# ----------------------------
# Thompson Sampling Agent
# ----------------------------
class ThompsonAgent:
    def __init__(self, action_size=10):
        self.alphas = np.ones(action_size)
        self.betas = np.ones(action_size)

    def update(self, action, reward):
        if reward == 1:
            self.alphas[action] += 1
        else:
            self.betas[action] += 1

    def get_action(self):
        samples = np.random.beta(self.alphas, self.betas)
        return np.argmax(samples)  

# ----------------------------
# Gradient Bandit Agent
# ----------------------------
class GradientAgent:
    def __init__(self, alpha=0.1, action_size=10, use_baseline=True):
        self.alpha = alpha
        self.H = np.zeros(action_size)      # preferences
        self.use_baseline = use_baseline
        self.avg_reward = 0.0
        self.t = 0

    def _policy(self):
        z = self.H - np.max(self.H)  # 안정화
        exp = np.exp(z)
        return exp / np.sum(exp)

    def get_action(self):
        probs = self._policy()
        return np.random.choice(len(self.H), p=probs)

    def update(self, action, reward):
        self.t += 1
        baseline = self.avg_reward if self.use_baseline else 0.0
        if self.use_baseline:
            self.avg_reward += (reward - self.avg_reward) / self.t
        probs = self._policy()
        for a in range(len(self.H)):
            if a == action:
                self.H[a] += self.alpha * (reward - baseline) * (1 - probs[a])
            else:
                self.H[a] -= self.alpha * (reward - baseline) * probs[a]

# ----------------------------
# Simulation
# ----------------------------
def run_experiment(agent_class, agent_kwargs, runs=200, steps=1000, action_size=10):
    all_rates = np.zeros((runs, steps))
    for run in range(runs):
        bandit = Bandit(action_size)
        agent = agent_class(**agent_kwargs)
        total_reward = 0
        rates = []
        for step in range(steps):
            action = agent.get_action()
            reward = bandit.play(action)
            agent.update(action, reward)
            total_reward += reward
            rates.append(total_reward / (step + 1))
        all_rates[run] = rates
    return np.mean(all_rates, axis=0)

# ----------------------------
# Run all algorithms
# ----------------------------
steps = 1000
avg_eps = run_experiment(EpsGreedyAgent, {"epsilon":0.1}, steps=steps)
avg_ucb = run_experiment(UCBAgent, {"c":2}, steps=steps)
avg_thompson = run_experiment(ThompsonAgent, {}, steps=steps)
avg_grad_baseline = run_experiment(GradientAgent, {"alpha":0.1, "use_baseline":True}, steps=steps)
avg_grad_no_baseline = run_experiment(GradientAgent, {"alpha":0.1, "use_baseline":False}, steps=steps)

# ----------------------------
# Plot
# ----------------------------
plt.figure(figsize=(10,6))
plt.plot(avg_eps, label="ε-greedy (ε=0.1)")
plt.plot(avg_ucb, label="UCB (c=2)")
plt.plot(avg_thompson, label="Thompson Sampling")
plt.plot(avg_grad_baseline, label="Gradient Bandit (α=0.1, baseline)")
plt.plot(avg_grad_no_baseline, label="Gradient Bandit (α=0.1, no baseline)")
plt.xlabel("Steps")
plt.ylabel("Average Reward Rate")
plt.legend()
plt.grid()
plt.show()
```

---
## A.2.2 Non-Stational
```python
import numpy as np
import matplotlib.pyplot as plt

# ----------------------------
# Non-Stationary Bandit
# ----------------------------
class NonStatBandit:
    def __init__(self, arms=10):
        self.arms = arms
        self.rates = np.random.rand(arms)  # 초기 확률
        
    def play(self, arm):
        rate = self.rates[arm]
        # 매 step마다 bandit 승률이 조금 변함
        self.rates += 0.01 * np.random.randn(self.arms)  
        self.rates = np.clip(self.rates, 0, 1)  # 확률은 [0,1]로 제한
        return 1 if np.random.rand() < rate else 0

# ----------------------------
# Alpha-based ε-greedy Agent (EMA update)
# ----------------------------
class AlphaEpsGreedyAgent:
    def __init__(self, epsilon, alpha, action_size=10):
        self.epsilon = epsilon
        self.alpha = alpha
        self.Qs = np.zeros(action_size)

    def update(self, action, reward):
        self.Qs[action] += self.alpha * (reward - self.Qs[action])

    def get_action(self):
        if np.random.rand() < self.epsilon:
            return np.random.randint(len(self.Qs))
        return np.argmax(self.Qs)

# ----------------------------
# Alpha-based UCB Agent
# ----------------------------
class AlphaUCBAgent:
    def __init__(self, c, alpha, action_size=10):
        self.c = c
        self.alpha = alpha
        self.Qs = np.zeros(action_size)
        self.ns = np.zeros(action_size) + 1e-5
        self.t = 0

    def update(self, action, reward):
        self.ns[action] += 1
        self.Qs[action] += self.alpha * (reward - self.Qs[action])

    def get_action(self):
        self.t += 1
        ucb_values = self.Qs + self.c * np.sqrt(np.log(self.t) / self.ns)
        return np.argmax(ucb_values)

# ----------------------------
# Thompson ampling Agent (Beta distribution 기반은 stationary 문제에 더 적합)
# Non-stationary에서는 sliding window 방식 또는 discounting 필요
# 여기서는 단순히 EMA 업데이트로 근사 버전 구현
# ----------------------------
class AlphaThompsonAgent:
    def __init__(self, alpha=0.1, action_size=10):
        self.alpha = alpha
        self.alphas = np.ones(action_size)
        self.betas = np.ones(action_size)
        
    def update(self, action, reward):
        # EMA 형태의 업데이트 (기존 Beta 업데이트 대신 discount 적용)
        self.alphas[action] = (1 - self.alpha) * self.alphas[action] + self.alpha * (1 + reward)
        self.betas[action] = (1 - self.alpha) * self.betas[action] + self.alpha * (1 + (1 - reward))

    def get_action(self):
        samples = np.random.beta(self.alphas, self.betas)
        return np.argmax(samples)

# ----------------------------
# Gradient Bandit Agent (softmax 정책, EMA 기반이라 non-stationary에도 적응 가능)
# ----------------------------
class GradientAgent:
    def __init__(self, alpha=0.1, action_size=10, use_baseline=True):
        self.alpha = alpha
        self.H = np.zeros(action_size)
        self.use_baseline = use_baseline
        self.avg_reward = 0.0
        self.t = 0

    def _policy(self):
        z = self.H - np.max(self.H)
        exp = np.exp(z)
        return exp / np.sum(exp)

    def get_action(self):
        probs = self._policy()
        return np.random.choice(len(self.H), p=probs)

    def update(self, action, reward):
        self.t += 1
        baseline = self.avg_reward if self.use_baseline else 0.0
        if self.use_baseline:
            self.avg_reward += (reward - self.avg_reward) / self.t
        probs = self._policy()
        for a in range(len(self.H)):
            if a == action:
                self.H[a] += self.alpha * (reward - baseline) * (1 - probs[a])
            else:
                self.H[a] -= self.alpha * (reward - baseline) * probs[a]

# ----------------------------
# Simulation Function
# ----------------------------
def run_experiment(agent_class, agent_kwargs, runs=200, steps=1000, action_size=10):
    all_rates = np.zeros((runs, steps))
    for run in range(runs):
        bandit = NonStatBandit(action_size)
        agent = agent_class(**agent_kwargs)
        total_reward = 0
        rates = []
        for step in range(steps):
            action = agent.get_action()
            reward = bandit.play(action)
            agent.update(action, reward)
            total_reward += reward
            rates.append(total_reward / (step + 1))
        all_rates[run] = rates
    return np.mean(all_rates, axis=0)
  
# ----------------------------
# Run Algorithms in Non-stationary Setting
# ----------------------------
steps = 1000
avg_eps = run_experiment(AlphaEpsGreedyAgent, {"epsilon":0.1, "alpha":0.1}, steps=steps)
avg_ucb = run_experiment(AlphaUCBAgent, {"c":2, "alpha":0.1}, steps=steps)
avg_thompson = run_experiment(AlphaThompsonAgent, {"alpha":0.1}, steps=steps)
avg_grad = run_experiment(GradientAgent, {"alpha":0.1, "use_baseline":True}, steps=steps)

# ----------------------------
# Plot
# ----------------------------
plt.figure(figsize=(10,6))
plt.plot(avg_eps, label="ε-greedy (ε=0.1, α=0.1)")
plt.plot(avg_ucb, label="UCB (c=2, α=0.1)")
plt.plot(avg_thompson, label="Thompson Sampling (EMA ver.)")
plt.plot(avg_grad, label="Gradient Bandit (α=0.1, baseline)")
plt.xlabel("Steps")
plt.ylabel("Average Reward Rate")
plt.title("Non-stationary Bandit Problem")
plt.legend()
plt.grid()
plt.show()
```