# 1) 상태 정의

감정은 보통 “라벨(기쁨/분노…)”만 쓰면 거칠어지기 때문에, **연속값 + 라벨**의 하이브리드가 좋음.

- **연속 상태(내부)**:
    
    - 정/부정(Valence) v ∈ [-1, 1]
        
    - 각성도(Arousal) a ∈ [0, 1]
        
    - 강도(Intensity) i ∈ [0, 1]
        
- **이산 라벨(외부표현용)**: mood ∈ {neutral, happy, excited, shy, angry, sad, tired …} -> 연속 상태에 따라 결정됨
    

→ 내부 연속상태가 실제 “정서 좌표”이고, 라벨은 렌더링(말투, 보이스, 표정)을 위한 스냅샷 이름

# 2) 관측치와 신호

매 틱(t)에 들어오는 신호 Oₜ는 대략 이렇게 봐.

- **텍스트 감성**: 사용자의 문장(칭찬/모욕/위로/하이프 등) → v, a 방향으로 미는 힘.
    
- **모델 보조신호**: LLM이 산출하는 `mood_delta`, `affinity_delta` 같은 메타.
    
- **관계 기억**: 사용자별 친밀도(affinity) → 장기적 긍·부정 편향.
    
- **이벤트 트리거**: 특정 단어/상황 → 라벨 전이를 강하게 힌트.
    

# 3) 전이 방정식의 뼈대

핵심 아이디어: “감정은 스르르 식고(감쇠), 자극엔 순간 반응하고(충격), 관계는 바이어스를 준다(편향)”

연속 상태 업데이트(Δt는 지난 시간, 클립은 범위 고정):

$$
\begin{aligned}
v_{t+1} &= (1-\lambda_v) \cdot v_t + \alpha_v \phi_v(O_t) + \beta \cdot \text{mood\_delta} + \eta \cdot \underbrace{\text{affinity}}_{\text{장기편향}} \\
a_{t+1} &= (1-\lambda_a) \cdot a_t + \alpha_a \phi_a(O_t) \\
i_{t+1} &= \text{clip}\Big((1-\lambda_i)\cdot i_t + \gamma \cdot (|v_{t+1}-v_t| + |a_{t+1}-a_t|)\Big)
\end{aligned}
$$

- **감쇠(λ)**: 시간이 지나면 원점(평정)으로 천천히 복귀 → 튐 방지, 잔향 표현.
    
- **자극(φᵥ, φₐ)**: 관측치에서 뽑은 “긍/부정, 흥분” 성분(키워드/이모지/감성분석 등).
    
- **보조신호(β·mood_delta)**: LLM이 상황맥락상 “조금 더 기쁘게/차분하게” 같은 힌트를 줄 때 쓰는 가중치.
    
- **친밀도(η·affinity)**: 같은 말이라도 친한 유저면 덜 화나고 더 기뻐지는 경향.
    

이후 **라벨 결정**은 ‘정서 원형(circumplex)’에 맞춰 사분면+임계로 뽑아.

- v↑ & a↑ → excited / happy
    
- v↑ & a↓ → happy / calm
    
- v↓ & a↑ → angry / anxious
    
- v↓ & a↓ → sad / tired
    
- 분류는 `argmax`(여러 후보 점수 중 최댓값) 또는 **룰 기반 힌트**(트리거가 있으면 우선).
    

여기에 두 가지 안정화 장치:

- **쿨다운**: 직전 라벨과 너무 빨리 바뀌지 않게 최소 유지시간(예: 2~4초).
    
- **히스테리시스**: 경계 근처에서 들쭉날쭉하지 않게 들어갈 때·나올 때 문턱을 다르게.
    

# 4) 왜 하이브리드가 좋은가

- **FSM(유한상태기계)/마르코프**만 쓰면: 전이표 만들기 쉬우나, 표현이 뻣뻣하고 블렌딩이 어려움.
    
- **연속형(PAD/VAD)**만 쓰면: 부드럽지만 “말투/보이스/표정”을 고르려면 결국 구간화가 필요.  
    → **연속 내부 + 이산 외부**가 실전(보이스/표정 매핑)에 가장 잘 맞아.
    

# 5) rei.json이 맡는 “이론적 역할”

`rei.json`은 **코드가 아니라 정책**이야. 즉, 파라미터/맵/룰을 **데이터로 선언**해서 엔진이 참조하게 하는 것.

담는 것:

1. **상태공간 정의**: 사용할 라벨 목록(emotions)
    
2. **엔진 하이퍼파라미터**: 감쇠율(λ), 쿨다운, 친밀도 바이어스(η), 강도 계산의 γ
    
3. **감성 사전**: praise/insult/comfort/hype… → φᵥ, φₐ에 들어갈 키워드 집합
    
4. **전이 트리거**: `"any->angry": ["바보", ...]` 같은 강한 힌트 룰
    
5. **렌더링 맵**:
    
    - `voice_map[mood]` → 보이스 ID, 말속도 등
        
    - `vts_parameters[mood]` → 표정 파라미터 세트
        
6. **프롬프트 힌트**: mood별 말투 지침(문장 길이, 이모티콘, 화법)
    

즉, **이론(상태-관측-전이-출력)**을 코드가 계산하고, **값/룰**은 JSON이 들고 있는 구조.

# 6) 숫자를 어떻게 잡나? (감각 → 수치화 가이드)

- **감쇠(λ)**는 “반감기”로 생각하면 편해.
    
    - 반감기 T_half이면, 1틱(1초라 치면) 감쇠율은 (\lambda = 1 - 2^{-1/T_{\text{half}}}).
        
    - 예: T_half=10초 ⇒ λ≈0.066 → 10초마다 영향이 절반으로.
        
- **자극 게인(αᵥ, αₐ)**: 보통 0.1~0.4 범위에서 시작. 키워드가 여러 개 맞으면 누적되게.
    
- **보조 게인(β)**: 모델 힌트는 “미세 조정” 용도 → 0.05~0.15 권장.
    
- **강도 계수(γ)**: v/a가 급히 바뀌면 i가 올라야 자연스러움 → 0.3~0.6 정도.
    
- **쿨다운**: 2~5초. 너무 길면 무딤, 너무 짧으면 깜빡임.
    
- **친밀 바이어스(η)**: affinity를 -100~+100으로 두면, η는 0.001~0.003 정도(느리게 누적).
    

# 7) 출력(말/보이스/표정)과의 이론적 연결

- **라벨**은 **선택**(voice preset, 표정 preset).
    
- **강도 i**는 **스케일링**에 써. 예:
    
    - 말속도/피치(보이스): arousal↑면 약간 빠르고 높게, sad/tired면 느리고 낮게.
        
    - 입모양/눈매(VTS): intensity·arousal로 MouthOpen/EyeWide 등을 보간.
        
- **valence**는 말투 단어 선택(긍정/부정 어휘, 감탄사 빈도)과 표정(웃음/찡그림)을 좌우.
    

# 8) 안정성 체크 포인트

- **진동**: 라벨이 자꾸 왕복하면 쿨다운↑, 히스테리시스 도입, λ↑.
    
- **포화**: i가 금방 1.0 근처면 γ↓ 또는 자극 누적 제한.
    
- **라벨 혼동**: sad↔tired 구분이 애매하면 경계값/우선순위 조정(예: a<0.25면 tired 우선).
    
- **사용자별 편향**: affinity가 너무 세면 금방 “항상 행복”이 되니 η 낮추기.
    

# 9) 확장 로드맵(선택)

- **감성 추정 고도화**: φᵥ, φₐ를 키워드 대신 문장 단위 감성모델(다국어, 이모티콘, 구어체)로 교체.
    
- **확률 모델**: HMM/칼만류 필터로 v,a를 추정(노이즈에 강함).
    
- **상황 기억**: 최근 N턴 요약(feeling context)로 자극을 시간평균.
    
- **개인화**: 유저별 α/λ/η를 점진 학습.
    

---

정리하면,

- 내부는 **연속 좌표(v,a,i)**가 “물리”처럼 움직이고(감쇠+자극+편향),
    
- 외부는 그걸 **라벨로 양자화**해서 보이스·표정을 뽑는다,
    
- 룰·숫자는 **rei.json**이 들고, 엔진은 그걸 읽어 **결정**만 한다—가 이론의 전부야.
    

여기까지 이해되면, 바로 그다음 단계는 “네 분위기에 맞는 초기 수치”를 찍어 넣고 반감기/쿨다운만 먼저 손보는 거야. 필요하면 반감기/쿨다운부터 같이 튜닝해보자.